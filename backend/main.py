import os
import json
import math
import logging

from dotenv import load_dotenv

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.params import Query
from pydantic import BaseModel, Field
from typing import List, Optional, Literal, Dict, Any, Union

#  Statistika
from scipy.stats import (
    boxcox, pearsonr, spearmanr, kendalltau, norm, shapiro, normaltest, kstest, zscore,
    ttest_ind, mannwhitneyu, f_oneway, kruskal, ttest_rel, wilcoxon, levene,
    chi2_contingency, fisher_exact
)
import statsmodels.api as sm
from statsmodels.discrete.discrete_model import MNLogit, Logit
from statsmodels.tools.sm_exceptions import PerfectSeparationError

#  Anal√Ωza faktor≈Ø a PCA
from factor_analyzer import FactorAnalyzer, calculate_bartlett_sphericity, calculate_kmo
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# üîç Machine Learning ‚Äì klasifikace, regrese, shlukov√°n√≠
from sklearn.linear_model import (
    LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_squared_error, r2_score, accuracy_score, precision_score,
    recall_score, f1_score, confusion_matrix, classification_report, silhouette_score
)
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# üßÆ Pr√°ce s daty
import pandas as pd
import numpy as np
import requests


app = FastAPI()
load_dotenv()


app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ClassificationMetricsInterpretation(BaseModel):
    accuracy: float
    precision: float # Weighted precision
    recall: float    # Weighted recall
    f1_score: float  # Weighted F1

class ClassificationInterpretationRequest(BaseModel):
    analysis_type: str = "classification"
    algorithm_used: str
    target_variable: str
    features_used: List[str]
    metrics: ClassificationMetricsInterpretation
    has_feature_importances: bool
    number_of_classes: int

# --- Nov√Ω endpoint pro AI interpretaci klasifikace ---
@app.post("/api/interpret_classification")
async def interpret_classification(req: ClassificationInterpretationRequest):
    # --- System Prompt pro LLM ---
    system_prompt = (
        "Jsi AI asistent specializuj√≠c√≠ se na anal√Ωzu dat. U≈æivatel provedl klasifikaƒçn√≠ anal√Ωzu a poskytne ti jej√≠ kl√≠ƒçov√© v√Ωsledky.\n\n"
        "Tv√Ωm √∫kolem je interpretovat tyto v√Ωsledky v ƒçe≈°tinƒõ, jednoduch√Ωm a srozumiteln√Ωm jazykem.\n\n"
        f"Pou≈æit√Ω algoritmus byl: **{req.algorithm_used}**. C√≠lov√° promƒõnn√° (kterou se sna≈æ√≠me predikovat) je '{req.target_variable}'. Poƒçet t≈ô√≠d c√≠lov√© promƒõnn√©: {req.number_of_classes}.\n\n"
        "**Interpretace kl√≠ƒçov√Ωch metrik (Accuracy, Precision, Recall, F1-Score):**\n"
        "- **Accuracy (P≈ôesnost):** Jak√© procento v≈°ech predikc√≠ bylo spr√°vn√Ωch? (nap≈ô. Accuracy 0.85 znamen√° 85% spr√°vn√Ωch predikc√≠ celkovƒõ). Je to dobr√° metrika, pokud jsou t≈ô√≠dy vyv√°≈æen√©.\n"
        "- **Precision (P≈ôesnost pozitivn√≠ch predikc√≠):** Z tƒõch p≈ô√≠pad≈Ø, kter√© model oznaƒçil jako pozitivn√≠ (pro danou t≈ô√≠du), kolik jich bylo skuteƒçnƒõ pozitivn√≠ch? Vysok√° precision znamen√° m√°lo fale≈°nƒõ pozitivn√≠ch v√Ωsledk≈Ø.\n"
        "- **Recall (Senzitivita, √öplnost):** Z tƒõch p≈ô√≠pad≈Ø, kter√© byly skuteƒçnƒõ pozitivn√≠ (pro danou t≈ô√≠du), kolik jich model spr√°vnƒõ identifikoval? Vysok√Ω recall znamen√° m√°lo fale≈°nƒõ negativn√≠ch v√Ωsledk≈Ø.\n"
        "- **F1-Score:** Harmonick√Ω pr≈Ømƒõr Precision a Recall. Dobr√° metrika, pokud hled√°me rovnov√°hu mezi Precision a Recall, nebo pokud jsou t≈ô√≠dy nevyv√°≈æen√©.\n"
        f"(Pozn√°mka: Poskytnut√© metriky Precision, Recall a F1 jsou v√°≈æen√© pr≈Ømƒõry p≈ôes v≈°echny {req.number_of_classes} t≈ô√≠dy, co≈æ zohled≈àuje jejich velikost.)\n\n"
        "**Celkov√© zhodnocen√≠ modelu:**\n"
        "- Na z√°kladƒõ hodnot metrik (typicky F1 nebo Accuracy) zhodno≈•, jak dob≈ôe model funguje. Hodnoty bl√≠zko 1 jsou ide√°ln√≠, hodnoty kolem 0.5 u bin√°rn√≠ klasifikace mohou znamenat, ≈æe model nen√≠ o moc lep≈°√≠ ne≈æ n√°hodn√© h√°d√°n√≠.\n"
        "- Zm√≠nit, ≈æe interpretace metrik z√°vis√≠ na kontextu probl√©mu (nap≈ô. v medic√≠nƒõ m≈Ø≈æe b√Ωt d≈Øle≈æitƒõj≈°√≠ vysok√Ω Recall ne≈æ Precision).\n\n"
        "**Dal≈°√≠ informace:**\n"
        f"- {'Model poskytl informaci o d≈Øle≈æitosti p≈ô√≠znak≈Ø (feature importances).' if req.has_feature_importances else 'Model neposkytl informaci o d≈Øle≈æitosti p≈ô√≠znak≈Ø.'} Pokud ano, znamen√° to, ≈æe nƒõkter√© vstupn√≠ promƒõnn√© mƒõly vƒõt≈°√≠ vliv na rozhodov√°n√≠ modelu ne≈æ jin√©.\n"
        "- Byla zm√≠nƒõna i Confusion Matrix (matice z√°mƒõn), kter√° detailnƒõ ukazuje, jak√© typy chyb model dƒõlal (kter√© t≈ô√≠dy si pletl s kter√Ωmi).\n\n"
        "Pravidla:\n"
        "- Odpov√≠dej v ƒçe≈°tinƒõ.\n"
        "- Buƒè srozumiteln√Ω pro nƒõkoho bez hlubok√Ωch znalost√≠ statistiky.\n"
        "- Vysvƒõtli v√Ωznam kl√≠ƒçov√Ωch metrik jednodu≈°e.\n"
        "- Neuv√°dƒõj vzorce ani k√≥d.\n"
        "- Form√°tuj odpovƒõƒè pro dobrou ƒçitelnost."
    )

    # --- Sestaven√≠ User Promptu ---
    user_prompt_parts = [
        f"Provedl jsem klasifikaƒçn√≠ anal√Ωzu pomoc√≠ algoritmu '{req.algorithm_used}'.",
        f"C√≠lov√° promƒõnn√°: '{req.target_variable}' ({req.number_of_classes} t≈ô√≠dy).",
        f"Pou≈æit√© p≈ô√≠znaky: {', '.join(req.features_used)}.",
        "\nSouhrnn√© metriky modelu (v√°≈æen√Ω pr≈Ømƒõr):"
        f"- P≈ôesnost (Accuracy): {req.metrics.accuracy:.4f}",
        f"- Precision: {req.metrics.precision:.4f}",
        f"- Recall: {req.metrics.recall:.4f}",
        f"- F1-Score: {req.metrics.f1_score:.4f}",
        f"\nModel {'poskytl' if req.has_feature_importances else 'neposkytl'} informaci o d≈Øle≈æitosti p≈ô√≠znak≈Ø.",
        "\nPros√≠m, interpretuj tyto v√Ωsledky."
    ]
    full_user_prompt = "\n".join(user_prompt_parts)

    # --- Vol√°n√≠ LLM API ---
    try:
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
             raise HTTPException(status_code=500, detail="Chyb√≠ konfigurace API kl√≠ƒçe pro LLM.")

        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek/deepseek-chat-v3-0324:free", # Nebo jin√Ω model
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_user_prompt}
                ],
                "max_tokens": 600
            },
            timeout=60
        )
        response.raise_for_status()

        llm_data = response.json()
        interpretation_text = llm_data.get("choices", [{}])[0].get("message", {}).get("content")

        if not interpretation_text:
             raise HTTPException(status_code=500, detail="AI nevr√°tila platnou interpretaci.")

        return {"interpretation": interpretation_text}

    except requests.exceptions.RequestException as req_err:
        logger.error(f"Chyba komunikace s LLM API: {req_err}")
        raise HTTPException(status_code=503, detail=f"Chyba p≈ôi komunikaci s AI slu≈æbou: {req_err}")
    except Exception as e:
        logger.error(f"Neoƒçek√°van√° chyba p≈ôi interpretaci klasifikace: {e}", exc_info=True)
        # print(f"User prompt was: {full_user_prompt}") # Pro debugging
        raise HTTPException(status_code=500, detail=f"Intern√≠ chyba serveru p≈ôi generov√°n√≠ interpretace: {str(e)}")
# Nastaven√≠ logov√°n√≠
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Nov√Ω Pydantic Model pro Klasifikaci ---
class ClassificationRequest(BaseModel):
    feature_columns: List[str] = Field(..., min_items=1)
    target_column: str
    algorithm: Literal["auto", "logistic_regression", "knn", "decision_tree", "random_forest", "naive_bayes"] = "auto"
    standardize: bool = True
    test_size: float = Field(default=0.25, ge=0.1, le=0.5) # Testovac√≠ sada 10-50%
    knn_neighbors: Optional[int] = Field(default=5, ge=1)
    random_state: int = 42 # Pro reprodukovatelnost

# --- Nov√Ω Endpoint pro Klasifikaci ---
@app.post("/api/classification_analysis")
async def classification_analysis(req: ClassificationRequest):
    logger.info(f"Classification request received: {req.dict()}")
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    headers = stored_datasets["latest"]["headers"]
    data = stored_datasets["latest"]["data"]
    df = pd.DataFrame(data, columns=headers)

    # --- Validace vstup≈Ø ---
    if req.target_column in req.feature_columns:
        raise HTTPException(status_code=400, detail="C√≠lov√° promƒõnn√° nem≈Ø≈æe b√Ωt z√°rove≈à p≈ô√≠znakem.")
    if not req.feature_columns:
         raise HTTPException(status_code=400, detail="Mus√≠te vybrat alespo≈à jeden p≈ô√≠znak (feature column).")

    all_selected_columns = req.feature_columns + [req.target_column]
    for col in all_selected_columns:
        if col not in df.columns:
            raise HTTPException(status_code=400, detail=f"Sloupec '{col}' nebyl v datech nalezen.")

    # --- P≈ô√≠prava dat ---
    try:
        df_subset = df[all_selected_columns].copy()

        # Z√°kladn√≠ ƒçi≈°tƒõn√≠ - odstranƒõn√≠ ≈ô√°dk≈Ø s jakoukoli chybƒõj√≠c√≠ hodnotou v vybran√Ωch sloupc√≠ch
        initial_rows = len(df_subset)
        df_subset.dropna(inplace=True)
        if len(df_subset) < initial_rows:
             logger.warning(f"Odstranƒõno {initial_rows - len(df_subset)} ≈ô√°dk≈Ø kv≈Øli chybƒõj√≠c√≠m hodnot√°m.")

        if df_subset.empty:
            raise ValueError("Po odstranƒõn√≠ chybƒõj√≠c√≠ch hodnot nez≈Østala ≈æ√°dn√° data.")

        # Kontrola c√≠lov√© promƒõnn√©
        target_series = df_subset[req.target_column]
        if target_series.nunique() < 2:
             raise ValueError(f"C√≠lov√° promƒõnn√° '{req.target_column}' mus√≠ m√≠t alespo≈à 2 unik√°tn√≠ hodnoty pro klasifikaci.")
        if target_series.nunique() > 50: # Varov√°n√≠ pro p≈ô√≠li≈° mnoho t≈ô√≠d
             logger.warning(f"C√≠lov√° promƒõnn√° '{req.target_column}' m√° {target_series.nunique()} unik√°tn√≠ch hodnot. To m≈Ø≈æe b√Ωt pro klasifikaci p≈ô√≠li≈° mnoho.")
        # Nep≈ôev√°d√≠me na kategorii explicitnƒõ, nech√°me sklearn, aby si poradil

        X = df_subset[req.feature_columns]
        y = target_series

        # Identifikace numerick√Ωch a kategorick√Ωch p≈ô√≠znak≈Ø
        numeric_features = X.select_dtypes(include=np.number).columns.tolist()
        categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()
        logger.info(f"Numeric features: {numeric_features}")
        logger.info(f"Categorical features: {categorical_features}")

        # --- Preprocessing Pipeline ---
        transformers = []
        if numeric_features:
            numeric_transformer_steps = []
            if req.standardize:
                numeric_transformer_steps.append(('scaler', StandardScaler()))
            # M≈Ø≈æeme p≈ôidat imputer, pokud bychom nechtƒõli dropovat NA
            # numeric_transformer_steps.append(('imputer', SimpleImputer(strategy='median')))
            if numeric_transformer_steps:
                 transformers.append(('num', Pipeline(steps=numeric_transformer_steps), numeric_features))

        if categorical_features:
            categorical_transformer = Pipeline(steps=[
                # ('imputer', SimpleImputer(strategy='most_frequent')), # Pro p≈ô√≠padn√© NA v kategori√≠ch
                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # sparse_output=False pro snaz≈°√≠ manipulaci
            ])
            transformers.append(('cat', categorical_transformer, categorical_features))

        if not transformers:
             raise ValueError("Nebyly nalezeny ≈æ√°dn√© vhodn√© p≈ô√≠znaky (numerick√© nebo kategorick√©) pro zpracov√°n√≠.")

        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough') # 'passthrough' zachov√° sloupce, kter√© nejsou explicitnƒõ transformov√°ny (pokud by nƒõjak√© byly)

        # Rozdƒõlen√≠ dat P≈òED aplikac√≠ preprocessingu (fit jen na train)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=req.test_size, random_state=req.random_state, stratify=y # Stratifikace je d≈Øle≈æit√°
        )
        logger.info(f"Train set size: {len(X_train)}, Test set size: {len(X_test)}")

    except ValueError as e:
         logger.error(f"Chyba p≈ôi p≈ô√≠pravƒõ dat: {e}", exc_info=True)
         raise HTTPException(status_code=400, detail=f"Chyba p≈ôi p≈ô√≠pravƒõ dat: {str(e)}")
    except Exception as e:
         logger.error(f"Neoƒçek√°van√° chyba p≈ôi p≈ô√≠pravƒõ dat: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"Neoƒçek√°van√° chyba serveru: {str(e)}")


    # --- V√Ωbƒõr a tr√©nov√°n√≠ modelu ---
    used_algorithm = req.algorithm
    reason = ""

    # Jednoduch√° auto logika (m≈Ø≈æe b√Ωt sofistikovanƒõj≈°√≠)
    if used_algorithm == "auto":
        if len(df_subset) < 1000 and not categorical_features:
             used_algorithm = "knn"
             reason = "Mal√Ω dataset bez kategorick√Ωch p≈ô√≠znak≈Ø -> KNN."
        elif categorical_features:
             used_algorithm = "random_forest" # Random Forest si dob≈ôe porad√≠ s mixem typ≈Ø
             reason = "Dataset obsahuje kategorick√© p≈ô√≠znaky -> Random Forest."
        else:
             used_algorithm = "logistic_regression"
             reason = "Standardn√≠ volba -> Logistick√° regrese."
        logger.info(f"Auto algorithm selected: {used_algorithm}")


    model_instance: Any # Pro typovou kontrolu
    if used_algorithm == "logistic_regression":
        model_instance = LogisticRegression(random_state=req.random_state, max_iter=1000) # Zv√Ω≈°en√≠ iterac√≠ pro konvergenci
    elif used_algorithm == "knn":
         if not numeric_features and not req.standardize:
             logger.warning("KNN pou≈æito bez numerick√Ωch p≈ô√≠znak≈Ø nebo standardizace. V√Ωsledky mohou b√Ωt neoptim√°ln√≠.")
         model_instance = KNeighborsClassifier(n_neighbors=req.knn_neighbors or 5)
    elif used_algorithm == "decision_tree":
        model_instance = DecisionTreeClassifier(random_state=req.random_state)
    elif used_algorithm == "random_forest":
        model_instance = RandomForestClassifier(random_state=req.random_state)
    elif used_algorithm == "naive_bayes":
         if categorical_features:
              # Pro mix typ≈Ø by byl lep≈°√≠ CategoricalNB nebo sm√≠≈°en√Ω p≈ô√≠stup, GaussianNB p≈ôedpokl√°d√° Gaussovsk√© rozdƒõlen√≠
              logger.warning("Pou≈æ√≠v√°te GaussianNB s kategorick√Ωmi p≈ô√≠znaky po OneHotEncode. Zva≈æte vhodnƒõj≈°√≠ Naive Bayes variantu pro sm√≠≈°en√° data.")
         if not req.standardize and numeric_features:
              logger.warning("GaussianNB pou≈æit bez standardizace numerick√Ωch p≈ô√≠znak≈Ø.")
         model_instance = GaussianNB()
    else:
         raise HTTPException(status_code=400, detail=f"Nezn√°m√Ω algoritmus: {used_algorithm}")


    # Vytvo≈ôen√≠ kompletn√≠ pipeline: Preprocessing -> Model
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                             ('classifier', model_instance)])

    try:
        # Tr√©nov√°n√≠ pipeline
        logger.info(f"Tr√©nov√°n√≠ modelu: {used_algorithm}...")
        pipeline.fit(X_train, y_train)
        logger.info("Tr√©nov√°n√≠ dokonƒçeno.")

        # Predikce na testovac√≠ch datech
        y_pred = pipeline.predict(X_test)
        logger.info("Predikce na testovac√≠ sadƒõ dokonƒçena.")

        # --- Vyhodnocen√≠ ---
        accuracy = accuracy_score(y_test, y_pred)
        # Pou≈æit√≠ 'weighted' pro pr≈Ømƒõrov√°n√≠ metrik v multi-class probl√©mech, 'macro' je dal≈°√≠ mo≈ænost
        # zero_division=0 zabr√°n√≠ chybƒõ, pokud nƒõkter√° t≈ô√≠da nem√° predikce/skuteƒçn√© hodnoty v test setu (co≈æ by bylo divn√©)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

        # Z√≠sk√°n√≠ n√°zv≈Ø t≈ô√≠d pro confusion matrix
        class_labels = sorted(y.unique().astype(str).tolist()) # Unik√°tn√≠ t≈ô√≠dy z p≈Øvodn√≠ch 'y', se≈ôazen√©
        cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_) # Pou≈æijeme t≈ô√≠dy z pipeline pro spr√°vn√© po≈ôad√≠
        cm_list = cm.tolist()

        # Classification report jako slovn√≠k
        report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0, labels=pipeline.classes_, target_names=[str(cls) for cls in pipeline.classes_])


        # --- Feature Importances (pokud jsou dostupn√©) ---
        feature_importances = None
        if hasattr(pipeline.named_steps['classifier'], 'feature_importances_'):
            importances = pipeline.named_steps['classifier'].feature_importances_
            # Z√≠sk√°n√≠ n√°zv≈Ø p≈ô√≠znak≈Ø PO transformaci (nap≈ô. po OneHotEncode)
            try:
                # Z√≠sk√°n√≠ transformer≈Ø z ColumnTransformer
                preprocessor_fitted = pipeline.named_steps['preprocessor']
                feature_names_out = preprocessor_fitted.get_feature_names_out()
                if len(importances) == len(feature_names_out):
                    feature_importances = [{"feature": name, "importance": round(float(imp), 4)}
                                           for name, imp in zip(feature_names_out, importances)]
                    # Se≈ôazen√≠ podle d≈Øle≈æitosti
                    feature_importances.sort(key=lambda x: x["importance"], reverse=True)
                else:
                    logger.warning("Nesoulad poƒçtu importances a n√°zv≈Ø p≈ô√≠znak≈Ø po transformaci.")
            except Exception as e:
                 logger.warning(f"Nepoda≈ôilo se z√≠skat n√°zvy p≈ô√≠znak≈Ø pro feature importances: {e}")


    except ValueError as e:
         # Specifick√© chyby z fit/predict/metrics
         logger.error(f"Chyba bƒõhem tr√©nov√°n√≠/vyhodnocen√≠: {e}", exc_info=True)
         raise HTTPException(status_code=400, detail=f"Chyba klasifikace: {str(e)}")
    except Exception as e:
         logger.error(f"Neoƒçek√°van√° chyba bƒõhem tr√©nov√°n√≠/vyhodnocen√≠: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"Neoƒçek√°van√° chyba serveru: {str(e)}")


    # --- Sestaven√≠ odpovƒõdi ---
    return {
        "algorithm_used": used_algorithm,
        "reason": reason,
        "standardized": req.standardize if numeric_features else None, # Relevantn√≠ jen pokud byly numerick√© feat.
        "test_size": req.test_size,
        "knn_neighbors": req.knn_neighbors if used_algorithm == "knn" else None,
        "feature_columns_used": req.feature_columns, # P≈Øvodn√≠ seznam
        "target_column": req.target_column,
        "metrics": {
            "accuracy": round(accuracy, 4),
            "precision_weighted": round(precision, 4),
            "recall_weighted": round(recall, 4),
            "f1_weighted": round(f1, 4),
        },
        "classification_report": report_dict, # Detailn√≠ report
        "confusion_matrix": cm_list,
        "confusion_matrix_labels": [str(cls) for cls in pipeline.classes_], # Labels pro osu matice
        "feature_importances": feature_importances # M≈Ø≈æe b√Ωt None
    }


class DependencyTestRequest(BaseModel):
    columns: List[str]
    method: Optional[str] = "auto"
    paired: Optional[bool] = False

def run_shapiro(series):
    series = series.dropna()
    if len(series) < 3 or len(series.unique()) < 2: return 0.0
    try:
        stat, p_value = shapiro(series)
        if np.isnan(p_value): return 0.0
        return p_value
    except Exception: return 0.0

def run_levene(*groups):
    valid_groups = [g.dropna() for g in groups if len(g.dropna()) >= 3]
    if len(valid_groups) < 2: return 0.0
    try:
        stat, p_value = levene(*valid_groups)
        if np.isnan(p_value): return 0.0
        return p_value
    except Exception: return 0.0



import pandas as pd
import numpy as np
import math
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
# Importy pro statistick√© testy
from scipy.stats import (
    chi2_contingency, # Pro Chi2 a oƒçek√°van√© frekvence
    fisher_exact,    # Pro Fisher≈Øv test
    shapiro,         # Test normality
    levene,          # Test homogenity rozptyl≈Ø
    ttest_ind,       # Nep√°rov√Ω t-test (vƒçetnƒõ Welchova)
    ttest_rel,       # P√°rov√Ω t-test
    mannwhitneyu,    # Mann-Whitney U (nep√°rov√Ω neparametrick√Ω)
    wilcoxon,        # Wilcoxon (p√°rov√Ω neparametrick√Ω)
    f_oneway,        # ANOVA
    kruskal          # Kruskal-Wallis
)
import logging # Pro lep≈°√≠ logov√°n√≠

# --- Pydantic modely ---
class DependencyTestRequest(BaseModel):
    columns: List[str]
    method: Optional[str] = "auto"
    paired: Optional[bool] = False

# --- FastAPI App (p≈ôedpokl√°d√°me existenci) ---
# app = FastAPI()

# --- Mock Data a Glob√°ln√≠ promƒõnn√© (p≈ôedpokl√°d√°me existenci) ---
# stored_datasets = { ... }

# --- Logging ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Pomocn√© funkce (P≈òEDPOKL√ÅD√ÅME JEJICH EXISTENCI NEBO DEFINICI V SOUBORU) ---
def run_shapiro(series):
    """Spust√≠ Shapiro-Wilk test, vrac√≠ p-hodnotu nebo 0.0 p≈ôi chybƒõ/m√°lo datech."""
    series = series.dropna()
    if len(series) < 3 or len(series.unique()) < 2:
        return 0.0 # Nelze testovat nebo nem√° smysl
    try:
        # O≈°et≈ôen√≠ konstantn√≠ch dat, kter√° mohou proj√≠t kontrolou unique, ale shapiro sel≈æe
        if series.std() < 1e-10: return 0.0
        stat, p_value = shapiro(series)
        if np.isnan(p_value): return 0.0
        return p_value
    except ValueError: return 0.0
    except Exception as e: logger.warning(f"Neoƒçek√°van√° chyba v shapiro: {e}"); return 0.0

def run_levene(*groups):
    """Spust√≠ Levene test, vrac√≠ p-hodnotu nebo 0.0 p≈ôi chybƒõ/m√°lo datech."""
    valid_groups = [g.dropna() for g in groups if len(g.dropna()) >= 3]
    if len(valid_groups) < 2: return 0.0
    try:
        # O≈°et≈ôen√≠, pokud maj√≠ nƒõkter√© skupiny nulov√Ω rozptyl
        if any(g.std() < 1e-10 for g in valid_groups if len(g)>0):
            logger.warning("Levene test: Nƒõkter√° skupina m√° nulov√Ω rozptyl.")
            # V tomto p≈ô√≠padƒõ nem≈Ø≈æeme p≈ôedpokl√°dat homogenitu
            return 0.0
        stat, p_value = levene(*valid_groups)
        if np.isnan(p_value): return 0.0
        return p_value
    except ValueError: return 0.0
    except Exception as e: logger.warning(f"Neoƒçek√°van√° chyba v levene: {e}"); return 0.0

def robust_clean_nan_inf(value: Any) -> Any:
    """Rekurzivnƒõ ƒçist√≠ data pro JSON, nahrazuje NaN/Inf za None, p≈ôev√°d√≠ numpy typy."""
    if isinstance(value, dict):
        return {k: robust_clean_nan_inf(v) for k, v in value.items()}
    elif isinstance(value, list):
        return [robust_clean_nan_inf(elem) for elem in value]
    elif isinstance(value, (np.ndarray, pd.Series)):
         # Zkontrolujeme dtype P≈òED konverz√≠ na list
         dtype_kind = value.dtype.kind
         cleaned_list = [robust_clean_nan_inf(elem) for elem in value.tolist()]
         # Pokud byl p≈Øvodn√≠ typ integer a v≈°echny hodnoty jsou None, vr√°t√≠me None list,
         # jinak by se mohly pr√°zdn√© listy interpretovat jako float
         if dtype_kind in 'iu' and all(x is None for x in cleaned_list):
             return cleaned_list
         # Zkus√≠me zachovat integery, pokud je to mo≈æn√©
         if dtype_kind in 'iu' and all(isinstance(x, int) or x is None for x in cleaned_list):
             return cleaned_list
         # Pokud obsahuje float nebo mix, p≈ôevedeme None na NaN a pak na float list
         # s None tam, kde byl p≈Øvodnƒõ None/NaN/Inf
         # Toto je komplexn√≠, mo≈æn√° je jednodu≈°≈°√≠ nechat to b√Ωt listem sm√≠≈°en√Ωch typ≈Ø
         return cleaned_list # Vr√°t√≠me list se zachovan√Ωmi None
    elif isinstance(value, (float, np.floating)):
        if pd.isna(value) or np.isinf(value): return None
        return float(value)
    elif isinstance(value, (int, np.integer)):
         if pd.isna(value): return None
         return int(value)
    elif isinstance(value, (bool, np.bool_)):
        return bool(value)
    elif isinstance(value, pd.Timestamp):
        return value.isoformat() if pd.notna(value) else None
    elif value is None or pd.isna(value):
        return None
    # Vr√°t√≠me hodnotu, pokud je to z√°kladn√≠ typ podporovan√Ω JSONem
    elif isinstance(value, (str,)):
        return value
    # Pro ostatn√≠ nepodporovan√© typy vr√°t√≠me jejich string reprezentaci nebo None
    try:
        json.dumps(value) # Zkus√≠me, jestli je JSON serializovateln√Ω
        return value
    except TypeError:
        logger.warning(f"Typ {type(value)} nen√≠ p≈ô√≠mo JSON serializovateln√Ω, vrac√≠m None.")
        return None
# --- Endpoint ---


@app.post("/api/dependency_test")
async def dependency_test(req: DependencyTestRequest):
    logger.info(f"Received dependency test request: Cols={req.columns}, Method='{req.method}', Paired={req.paired}")
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=404, detail="Data nejsou nahran√°.")

    headers = stored_datasets["latest"].get("headers")
    data = stored_datasets["latest"].get("data")
    # Z√≠sk√°n√≠ typ≈Ø sloupc≈Ø
    raw_types = stored_datasets["latest"].get("column_types", {})
    if not raw_types:
         logger.warning("Chyb√≠ info o typech sloupc≈Ø, detekuji...")
         df_temp = pd.DataFrame(data, columns=headers)
         raw_types = {}
         for col in headers:
             try: pd.to_numeric(df_temp[col], errors='raise'); raw_types[col] = {"type": "Numeric"}
             except: raw_types[col] = {"type": "Categorical"}
         logger.info(f"Detekovan√© typy: {raw_types}")

    types = {col: info.get("type", "Unknown").replace("ƒå√≠seln√Ω","Numeric").replace("Kategorie","Categorical") for col, info in raw_types.items()}
    df = pd.DataFrame(data, columns=headers)

    selected_cols = req.columns
    method = (req.method or "auto").lower()
    paired = req.paired or False

    if len(selected_cols) < 2: raise HTTPException(status_code=400, detail="Je t≈ôeba vybrat alespo≈à dvƒõ promƒõnn√©.")
    if not all(col in headers for col in selected_cols):
        missing = [col for col in selected_cols if col not in headers]; raise HTTPException(status_code=404, detail=f"Sloupce nenalezeny: {missing}")

    # --- P≈ô√≠prava dat a logika v jednom try bloku ---
    try:
        # Inicializace v√Ωstupn√≠ho slovn√≠ku hned na zaƒç√°tku
        test_output: Dict[str, Any] = {
            "input_method": method, "input_paired": paired, "test_name": None,
            "reason": f"Metoda: {method}.", "columns": selected_cols, "results": [],
            "statistic": None, "p_value": None, "statistic_name": None,
            "degrees_freedom": None, "contingency_table": None, "warning_message": None
        }
        actual_method = method # V√Ωchoz√≠ metoda

        # P≈ô√≠prava dat
        subdf = df[selected_cols].copy()
        n_selected = len(selected_cols)
        cat_cols_in_scope = [col for col in selected_cols if types.get(col) == "Categorical"]
        num_cols_in_scope = [col for col in selected_cols if types.get(col) == "Numeric"]
        n_cat = len(cat_cols_in_scope); n_num = len(num_cols_in_scope)

        if n_cat + n_num != n_selected:
            unknown_cols = [col for col in selected_cols if types.get(col, "Unknown") == "Unknown"]; logger.warning(f"Nezn√°m√Ω typ pro: {unknown_cols}.")

        logger.info(f"Selected: {n_selected} total, {n_cat} cat, {n_num} num")

        if n_num > 0: subdf[num_cols_in_scope] = subdf[num_cols_in_scope].apply(pd.to_numeric, errors='coerce')
        if n_cat > 0: subdf[cat_cols_in_scope] = subdf[cat_cols_in_scope].astype('category')

        initial_rows = len(subdf); subdf.dropna(inplace=True); final_rows = len(subdf)
        logger.info(f"Data prep: Removed {initial_rows - final_rows} NA rows. Final rows: {final_rows}")
        if final_rows < 5: raise ValueError(f"Nedostatek dat po odstranƒõn√≠ NA (nalezeno: {final_rows}, min 5).")

        # --- Logika 'auto' v√Ωbƒõru ---
        if method == "auto":
            logger.info("AUTO method selected. Determining test...")
            reason = "Auto v√Ωbƒõr: "
            AUTO_CHI2, AUTO_FISHER = "chi2", "fisher"
            AUTO_TTEST_PAIRED, AUTO_TTEST_UNPAIRED_EQUAL, AUTO_TTEST_UNPAIRED_WELCH = "t.test.paired", "t.test.unpaired.equal", "t.test.unpaired.welch"
            AUTO_WILCOXON, AUTO_MANNWHITNEY = "wilcoxon", "mannwhitney"
            AUTO_ANOVA, AUTO_KRUSKAL = "anova", "kruskal"

            if n_cat == 2 and n_num == 0:
                cat1, cat2 = cat_cols_in_scope
                try:
                    cont_table = pd.crosstab(subdf[cat1], subdf[cat2])
                    if cont_table.size < 4 or cont_table.shape[0] < 2 or cont_table.shape[1] < 2: raise ValueError("Tabulka < 2x2.")
                    chi2, p, dof, expected = chi2_contingency(cont_table)
                    if np.any(expected < 5):
                        actual_method = AUTO_FISHER; reason += "2 Kat. (oƒçek. < 5) -> Fisher."
                        if cont_table.shape != (2, 2): actual_method = AUTO_CHI2; reason = reason.replace("-> Fisher.", "-> Chi2 (tabulka nen√≠ 2x2).")
                    else: actual_method = AUTO_CHI2; reason += "2 Kat. (oƒçek. >= 5) -> œá¬≤."
                except ValueError as e: raise ValueError(f"Nelze vytvo≈ôit kont. tabulku pro {cat1} vs {cat2}: {e}") from e
            elif n_cat > 2 and n_num == 0: actual_method = AUTO_CHI2; reason += f">2 Kat. -> œá¬≤ (mezi {cat_cols_in_scope[0]} vs {cat_cols_in_scope[1]})."
            elif n_cat == 1 and n_num == 1:
                cat_col, num_col = cat_cols_in_scope[0], num_cols_in_scope[0]
                num_levels = subdf[cat_col].nunique();
                if num_levels < 2: raise ValueError(f"Kat. '{cat_col}' < 2 √∫rovnƒõ.")
                groups = [group[num_col] for name, group in subdf.groupby(cat_col)]
                if num_levels == 2:
                    normality_p1 = run_shapiro(groups[0]); normality_p2 = run_shapiro(groups[1]); levene_p = run_levene(*groups)
                    normality_ok = normality_p1 > 0.05 and normality_p2 > 0.05; homogeneity_ok = levene_p > 0.05
                    reason += f"1Kat({num_levels})+1Num. Shapiro p=({normality_p1:.3f},{normality_p2:.3f}). Levene p={levene_p:.3f}. "
                    if normality_ok and homogeneity_ok: actual_method = AUTO_TTEST_UNPAIRED_EQUAL; reason += "-> t-test (nep√°r., shod. rozpt.)"
                    elif normality_ok and not homogeneity_ok: actual_method = AUTO_TTEST_UNPAIRED_WELCH; reason += "-> Welch≈Øv t-test (nep√°r., r≈Øz. rozpt.)"
                    else: actual_method = AUTO_MANNWHITNEY; reason += "-> Mann-Whitney U"
                else: # K > 2
                    normality_p_overall = run_shapiro(subdf[num_col]); levene_p = run_levene(*groups)
                    normality_ok = normality_p_overall > 0.05; homogeneity_ok = levene_p > 0.05
                    reason += f"1Kat({num_levels})+1Num. Shapiro(celk.) p={normality_p_overall:.3f}. Levene p={levene_p:.3f}. "
                    if normality_ok and homogeneity_ok: actual_method = AUTO_ANOVA; reason += "-> ANOVA"
                    else: actual_method = AUTO_KRUSKAL; reason += "-> Kruskal-Wallis"
            elif n_cat == 1 and n_num > 1: actual_method = AUTO_ANOVA; reason += f"1 Kat. + {n_num} Num. -> ANOVA (pro ka≈æd√Ω num. vs kat.)"
            elif n_cat == 0 and n_num == 2:
                num1_col, num2_col = num_cols_in_scope
                normality_p1 = run_shapiro(subdf[num1_col]); normality_p2 = run_shapiro(subdf[num2_col])
                normality_ok = normality_p1 > 0.05 and normality_p2 > 0.05
                reason += f"2 Num. Shapiro p=({normality_p1:.3f},{normality_p2:.3f}). P√°rov√° data={paired}. "
                if normality_ok:
                    actual_method = AUTO_TTEST_PAIRED if paired else "t.test.unpaired.auto" # Zkontroluje Levene pozdƒõji
                    reason += f"-> {('P√°rov√Ω' if paired else 'Nep√°rov√Ω')} t-test" + (" (Student/Welch dle Levene)" if not paired else "")
                else: actual_method = AUTO_WILCOXON if paired else AUTO_MANNWHITNEY; reason += f"-> {('Wilcoxon≈Øv p√°rov√Ω' if paired else 'Mann-Whitney U')}"
            else: raise ValueError("Automatick√Ω v√Ωbƒõr pro tuto kombinaci typ≈Ø promƒõnn√Ωch nen√≠ podporov√°n.")

            logger.info(f"AUTO selected method code: {actual_method}")
            test_output["reason"] = reason.strip()

        # --- Proveden√≠ vybran√©ho testu ---
        test_output["test_name"] = actual_method # Skuteƒçnƒõ pou≈æit√Ω k√≥d metody
        stat, p, dof = float('nan'), float('nan'), None # Inicializace pro testy vracej√≠c√≠ 1 v√Ωsledek

        if actual_method == "chi2":
            if n_cat < 2: raise ValueError("œá¬≤ test vy≈æaduje >= 2 kat.")
            cat1, cat2 = cat_cols_in_scope[0], cat_cols_in_scope[1]
            cont_table = pd.crosstab(subdf[cat1], subdf[cat2])
            if cont_table.size < 4 or cont_table.shape[0] < 2 or cont_table.shape[1] < 2: raise ValueError("Kontingenƒçn√≠ tabulka < 2x2.")
            stat, p, dof, expected = chi2_contingency(cont_table)
            test_output["test_name"] = "Chi-squared (œá¬≤)"; test_output["statistic"] = stat; test_output["p_value"] = p; test_output["degrees_freedom"] = dof; test_output["statistic_name"] = "œá¬≤"; test_output["contingency_table"] = cont_table.to_dict()

        elif actual_method == "fisher":
             if n_cat != 2: raise ValueError("Fisher vy≈æaduje 2 kat.")
             cat1, cat2 = cat_cols_in_scope
             cont_table = pd.crosstab(subdf[cat1], subdf[cat2])
             if cont_table.shape != (2, 2):
                 logger.warning(f"Fisher byl zvolen pro ne-2x2 tabulku ({cat1} vs {cat2}). Pou≈æ√≠v√°m Chi2.")
                 test_output["reason"] += " (Varov√°n√≠: Tabulka nen√≠ 2x2, pou≈æit Chi2!)"
                 stat, p, dof, expected = chi2_contingency(cont_table)
                 test_output["test_name"] = "Chi-squared (œá¬≤ - fallback)"; test_output["statistic"] = stat; test_output["p_value"] = p; test_output["degrees_freedom"] = dof; test_output["statistic_name"] = "œá¬≤"; test_output["contingency_table"] = cont_table.to_dict()
             else:
                 odds_ratio, p = fisher_exact(cont_table.values); test_output["test_name"] = "Fisher≈Øv p≈ôesn√Ω test"; test_output["statistic"] = odds_ratio; test_output["p_value"] = p; test_output["statistic_name"] = "Odds Ratio"; test_output["contingency_table"] = cont_table.to_dict()

        elif actual_method in ["anova", "kruskal"]:
             if n_cat < 1 or n_num < 1: raise ValueError(f"{actual_method.upper()} vy≈æaduje >=1 kat. a >=1 num.")
             results_list = []
             for cat_col in cat_cols_in_scope:
                 if subdf[cat_col].nunique() < 2: continue
                 groups_base = subdf.groupby(cat_col)
                 for num_col in num_cols_in_scope:
                     groups = [group[num_col].dropna() for name, group in groups_base if not group[num_col].dropna().empty]
                     if len(groups) < 2: continue
                     stat, p = float('nan'), float('nan')
                     try:
                         if actual_method == "anova": stat, p = f_oneway(*groups)
                         elif actual_method == "kruskal": stat, p = kruskal(*groups)
                     except ValueError as test_err: logger.warning(f"Test {actual_method} selhal pro {num_col} vs {cat_col}: {test_err}")
                     results_list.append({"cat_col": cat_col, "num_col": num_col, "statistic": stat if pd.notna(stat) else None, "p_value": p if pd.notna(p) else None})
             if not results_list: raise ValueError(f"{actual_method.upper()} nemohla b√Ωt provedena.")
             test_output["test_name"] = "ANOVA" if actual_method == "anova" else "Kruskal-Wallis"; test_output["results"] = results_list

        elif actual_method.startswith("t.test"):
            if n_num != 2 or n_cat != 0: raise ValueError("t-test vy≈æaduje 2 num.")
            num1_col, num2_col = num_cols_in_scope
            num1 = subdf[num1_col]; num2 = subdf[num2_col]
            if actual_method == "t.test.paired":
                stat, p = ttest_rel(num1, num2, nan_policy='omit'); test_output["test_name"] = "P√°rov√Ω t-test"; test_output["statistic_name"] = "t"
            else: # Nep√°rov√©
                equal_var_flag = False # Default Welch
                if actual_method == "t.test.unpaired.equal": equal_var_flag = True
                elif actual_method == "t.test.unpaired.welch": equal_var_flag = False
                elif actual_method == "t.test.unpaired.auto": # Znovu Levene
                     levene_p = run_levene(num1, num2); equal_var_flag = levene_p > 0.05
                     test_output["reason"] += f" Levene p={levene_p:.3f}."
                elif method == "t.test": # Manu√°ln√≠ volba 't.test'
                     levene_p = run_levene(num1, num2); equal_var_flag = levene_p > 0.05
                     test_output["reason"] += f" (Detekce rozptyl≈Ø: Levene p={levene_p:.3f})."
                stat, p = ttest_ind(num1, num2, equal_var=equal_var_flag, nan_policy='omit')
                test_output["test_name"] = "Student≈Øv t-test (nep√°rov√Ω)" if equal_var_flag else "Welch≈Øv t-test (nep√°rov√Ω)"; test_output["statistic_name"] = "t"
            test_output["statistic"] = stat; test_output["p_value"] = p

        elif actual_method == "wilcoxon":
            if n_num != 2 or n_cat != 0 or not paired: raise ValueError("Wilcoxon vy≈æaduje 2 num p√°rov√©.")
            num1_col, num2_col = num_cols_in_scope; stat, p = wilcoxon(subdf[num1_col], subdf[num2_col], zero_method='zsplit', correction=True, mode='approx', nan_policy='omit')
            test_output["test_name"] = "Wilcoxon≈Øv p√°rov√Ω test"; test_output["statistic"] = stat; test_output["p_value"] = p; test_output["statistic_name"] = "W"

        elif actual_method == "mannwhitney":
             if n_num == 2 and n_cat == 0 and not paired:
                 num1_col, num2_col = num_cols_in_scope; stat, p = mannwhitneyu(subdf[num1_col], subdf[num2_col], alternative='two-sided', nan_policy='omit')
                 test_output["test_name"] = "Mann-Whitney U test"
             elif n_num == 1 and n_cat == 1:
                  cat_col, num_col = cat_cols_in_scope[0], num_cols_in_scope[0];
                  if subdf[cat_col].nunique() != 2: raise ValueError("Mann-Whitney pro kat. vs num. vy≈æaduje 2 √∫rovnƒõ kat.")
                  groups = [group[num_col].dropna() for name, group in subdf.groupby(cat_col)]
                  if len(groups) != 2: raise ValueError("Nepoda≈ôilo se rozdƒõlit do dvou skupin.")
                  stat, p = mannwhitneyu(groups[0], groups[1], alternative='two-sided', nan_policy='omit')
                  test_output["test_name"] = "Mann-Whitney U test"
             else: raise ValueError("Neplatn√° kombinace pro Mann-Whitney U.")
             test_output["statistic"] = stat; test_output["p_value"] = p; test_output["statistic_name"] = "U"

        else:
             if method != "auto": raise HTTPException(status_code=400, detail=f"Metoda '{method}' nen√≠ implementov√°na.")
             else: raise NotImplementedError(f"Intern√≠ chyba: Metoda '{actual_method}' z 'auto' nen√≠ implementov√°na.")

        # --- Fin√°ln√≠ ƒçi≈°tƒõn√≠ NaN/Inf pro JSON ---
        final_output = robust_clean_nan_inf(test_output)
        logger.info("Dependency test finished successfully.")
        return final_output

    except (ValueError, KeyError) as user_err:
        logger.error(f"Error during dependency test: {user_err}", exc_info=True)
        raise HTTPException(status_code=400, detail=f"Chyba anal√Ωzy z√°vislosti: {str(user_err)}")
    except HTTPException as http_err:
         logger.warning(f"HTTP Exception during dependency test: {http_err.detail}")
         raise http_err
    except Exception as e:
        logger.exception("--- UNEXPECTED ERROR during Dependency Test ---")
        raise HTTPException(status_code=500, detail=f"Neoƒçek√°van√° intern√≠ chyba serveru p≈ôi testu z√°vislosti: {str(e)}")

# ... (zbytek souboru main.py) ...

class RegressionRequest(BaseModel):
    y: str
    x: List[str]
    method: Optional[str] = "auto"

# --- Helper funkce pro form√°tov√°n√≠ (podobn√© R) ---
def format_p_value(p_val):
    if pd.isna(p_val):
        return '-'
    if p_val < 0.001:
        return "<0.001" # Jednodu≈°≈°√≠ form√°tov√°n√≠ ne≈æ vƒõdeck√° notace
    return f"{p_val:.3f}"

def format_coef(coef):
     if pd.isna(coef):
         return '-'
     # Zobraz√≠ v√≠ce m√≠st pro mal√© koeficienty, m√©nƒõ pro velk√©
     if abs(coef) < 0.0001:
         return f"{coef:.4e}"
     elif abs(coef) < 1:
          return f"{coef:.4f}"
     else:
         return f"{coef:.3f}"


logging.basicConfig(level=logging.INFO) # Zobraz√≠ INFO a vy≈°≈°√≠ (WARNING, ERROR, CRITICAL)
logger = logging.getLogger(__name__)
# Nastaven√≠ vy≈°≈°√≠ √∫rovnƒõ pro knihovny, aby n√°s neru≈°ily, pokud nechceme
logging.getLogger("uvicorn").setLevel(logging.WARNING)
logging.getLogger("statsmodels").setLevel(logging.WARNING)

def robust_clean_nan_inf(value: Any) -> Any:
    """
    Rekurzivnƒõ ƒçist√≠ data pro JSON serializaci.
    Nahrazuje NaN/Inf hodnotami None.
    P≈ôev√°d√≠ numpy ƒç√≠seln√© typy na standardn√≠ Python typy.
    Zachov√°v√° None.
    """
    if isinstance(value, dict):
        return {k: robust_clean_nan_inf(v) for k, v in value.items()}
    elif isinstance(value, list):
        return [robust_clean_nan_inf(elem) for elem in value]
    elif isinstance(value, (np.ndarray, pd.Series)):
         cleaned_list = [robust_clean_nan_inf(elem) for elem in value.tolist()]
         return cleaned_list
    elif isinstance(value, (float, np.floating)):
        # Handles NaN and Inf specifically
        if pd.isna(value) or np.isinf(value): # Use pd.isna for broader check
            return None
        return float(value)
    elif isinstance(value, (int, np.integer)):
         # Handle potential pd.NA in integer columns converted by pandas > 1.0
         if pd.isna(value):
             return None
         return int(value)
    elif isinstance(value, (bool, np.bool_)):
        return bool(value)
    elif isinstance(value, pd.Timestamp):
        return value.isoformat() if pd.notna(value) else None
    elif value is None or pd.isna(value): # Catch None and pd.NA
        return None
    elif isinstance(value, str):
         return value
    # For other types supported by JSON (like standard Python numbers), return as is
    return value
    # --- Upraven√Ω Endpoint ---


@app.post("/api/regression_analysis")
async def regression_analysis(req: RegressionRequest):
    # --- Naƒçten√≠ dat a z√°kladn√≠ validace (stejn√© jako p≈ôedt√≠m) ---
    logger.info(f"Received regression request (statsmodels for OLS/Logit/MNLogit): Y='{req.y}', X={req.x}, Method='{req.method}'")
    global stored_datasets
    if "latest" not in stored_datasets: raise HTTPException(status_code=400, detail="Data nenahr√°na.")
    headers = stored_datasets["latest"].get("headers")
    data = stored_datasets["latest"].get("data")
    if not headers or not data: raise HTTPException(status_code=400, detail="Data nekompletn√≠.")
    df = pd.DataFrame(data, columns=headers)
    logger.info(f"Loaded data. Shape: {df.shape}")
    y_col = req.y; x_cols = req.x; method = req.method.lower() if req.method else "auto"
    if y_col not in df.columns: raise HTTPException(status_code=400, detail=f"Y sloupec '{y_col}' nenalezen.")
    invalid_x = [col for col in x_cols if col not in df.columns];
    if invalid_x: raise HTTPException(status_code=400, detail=f"X sloupce {invalid_x} nenalezeny.")
    if y_col in x_cols: raise HTTPException(status_code=400, detail="Y nem≈Ø≈æe b√Ωt v X.")

    # --- P≈ô√≠prava dat (udr≈æujeme X jako DataFrame) ---
    try:
        relevant_cols = [y_col] + x_cols
        df_subset = df[relevant_cols].replace('', np.nan).copy()
        for col in relevant_cols: df_subset[col] = pd.to_numeric(df_subset[col], errors='coerce')
        initial_rows = len(df_subset); df_subset.dropna(inplace=True); final_rows = len(df_subset)
        logger.info(f"Data prep: Removed {initial_rows - final_rows} rows with NA. Final rows: {final_rows}")
        if final_rows < len(relevant_cols) + 1: raise ValueError(f"Nedostatek dat ({final_rows}).")

        y_pd = df_subset[y_col]
        X_pd = df_subset[x_cols]
        y_unique_prepared = y_pd.unique()
        if len(y_unique_prepared) < 2: raise ValueError("Y m√° m√©nƒõ ne≈æ 2 unik√°tn√≠ hodnoty.")

        # Detekce typu Y a v√Ωbƒõr metody 'auto' (stejn√° logika jako p≈ôedt√≠m)
        y_is_numeric = pd.api.types.is_numeric_dtype(y_pd.dtype)
        y_is_likely_binary = len(y_unique_prepared) == 2
        y_is_likely_multicat = (not y_is_numeric or pd.api.types.is_integer_dtype(y_pd.dtype)) and \
                               len(y_unique_prepared) > 2 and len(y_unique_prepared) <= 15 # Prah z≈Øst√°v√°
        selected_method = method; reason = ""
        if method == "auto":
            if y_is_likely_binary: selected_method = "logistic"; reason = "Auto: Y m√° 2 √∫rovnƒõ -> Logistick√° (Logit)."
            elif y_is_likely_multicat: selected_method = "multinomial"; reason = "Auto: Y kat. (>2 √∫rovnƒõ) -> Multinomi√°ln√≠ (MNLogit)."
            elif y_is_numeric:
                 n_samples, n_features = X_pd.shape; strong_corr = False
                 if n_features > 1:
                      try:
                          corr_matrix = X_pd.corr().abs()
                          upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
                          strong_corr = (upper_tri > 0.7).any().any()
                      except Exception: pass
                 if n_features >= n_samples: selected_method = "ridge"; reason = "Auto: p >= n -> Ridge."
                 elif strong_corr: selected_method = "ridge"; reason = "Auto: Siln√° korelace X -> Ridge."
                 else: selected_method = "ols"; reason = "Auto: Standardn√≠ numerick√© Y -> OLS."
            else: raise ValueError(f"Nelze auto. urƒçit metodu pro Y typu {y_pd.dtype}.")
        else: # Manu√°ln√≠ volba + validace
             if method == "logistic" and not y_is_likely_binary: raise HTTPException(status_code=400, detail="Logistick√° vy≈æaduje Y se 2 √∫rovnƒõmi.")
             if method == "multinomial" and not y_is_likely_multicat:
                 if not pd.api.types.is_string_dtype(y_pd.dtype) and not pd.api.types.is_object_dtype(y_pd.dtype):
                      raise HTTPException(status_code=400, detail="Multinomi√°ln√≠ vy≈æaduje kategorick√© Y.")
             if method in ["ols", "ridge", "lasso", "elasticnet"] and not y_is_numeric: raise HTTPException(status_code=400, detail=f"Metoda '{method}' vy≈æaduje numerick√© Y.")
             selected_method = method
             reason = f"Metoda zvolena u≈æivatelem: {method.upper()}"

        logger.info(f"Selected method: {selected_method}. Reason: {reason}")
        model_results = {"method": selected_method, "reason": reason}

    except (ValueError, KeyError) as prep_err:
         logger.error(f"Error during data preparation: {prep_err}", exc_info=True)
         raise HTTPException(status_code=400, detail=f"Chyba p≈ôi p≈ô√≠pravƒõ dat: {str(prep_err)}")
    except Exception as e:
         logger.exception("--- UNEXPECTED ERROR DURING DATA PREPARATION ---")
         raise HTTPException(status_code=500, detail=f"Neoƒçek√°van√° chyba serveru p≈ôi p≈ô√≠pravƒõ dat: {str(e)}")

    # --- Fitov√°n√≠ a Zpracov√°n√≠ V√Ωsledk≈Ø ---
    try:
        # --- OLS pomoc√≠ STATSMODELS ---
        if selected_method == "ols":
            logger.info("Using statsmodels.OLS for analysis.")
            y_sm = y_pd.astype(float)
            X_sm_df = X_pd.astype(float)
            X_sm = sm.add_constant(X_sm_df, has_constant='raise')
            model = sm.OLS(y_sm, X_sm)
            results = model.fit()
            logger.info("Statsmodels OLS model fitted.")

            # Extrakce metrik a statistik (jako ƒç√≠sla)
            model_results["r2"] = getattr(results, 'rsquared', None)
            model_results["r2_adjusted"] = getattr(results, 'rsquared_adj', None)
            model_results["mse"] = getattr(results, 'mse_resid', None)
            model_results["rmse"] = np.sqrt(model_results["mse"]) if pd.notna(model_results["mse"]) and model_results["mse"] >= 0 else None
            model_results["f_statistic"] = getattr(results, 'fvalue', None)
            model_results["f_pvalue"] = getattr(results, 'f_pvalue', None) # p-hodnota F-testu
            model_results["n_observations"] = int(getattr(results, 'nobs', 0))
            model_results["intercept"] = results.params.get('const')
            model_results["accuracy"] = None # Nen√≠ relevantn√≠

            coeffs_list = []
            params = results.params.drop('const', errors='ignore')
            p_values = results.pvalues.drop('const', errors='ignore')
            try: conf_int_df = results.conf_int()
            except Exception: conf_int_df = pd.DataFrame(index=params.index, columns=[0, 1], data=np.nan)
            bse = results.bse.drop('const', errors='ignore') # Std. Error
            tvalues = results.tvalues.drop('const', errors='ignore') # t-value

            for name in params.index:
                coeffs_list.append({
                    "name": name,
                    "coef": params.get(name),
                    "stderr": bse.get(name),
                    "t_value": tvalues.get(name),
                    "p_value": p_values.get(name),
                    "ciLow": conf_int_df.loc[name, 0] if name in conf_int_df.index else None,
                    "ciHigh": conf_int_df.loc[name, 1] if name in conf_int_df.index else None,
                })
            model_results["coefficients"] = coeffs_list

            # Data pro grafy
            y_pred = getattr(results, 'fittedvalues', None)
            residuals_val = getattr(results, 'resid', None)
            X_aligned = X_pd # X u≈æ je DataFrame se spr√°vn√Ωm indexem
            if X_aligned.shape[1] == 1 and y_pred is not None and not y_pred.empty:
                 model_results["scatter_data"] = {"x": X_aligned.iloc[:, 0].tolist(), "y_true": y_sm.tolist(), "y_pred": y_pred.tolist()}
            else: model_results["scatter_data"] = None
            if y_pred is not None and not y_pred.empty and residuals_val is not None and not residuals_val.empty:
                 model_results["residuals"] = {"predicted": y_pred.tolist(), "residuals": residuals_val.tolist()}
            else: model_results["residuals"] = None

            model_results["note"] = "V√Ωsledky z OLS (statsmodels)."

        # --- LOGISTICK√Å REGRESE (bin√°rn√≠) pomoc√≠ STATSMODELS ---
        elif selected_method == "logistic":
            logger.info("Using statsmodels.Logit for analysis.")
            # Y mus√≠ b√Ωt numerick√© (0/1), pokud nen√≠, zkus√≠me p≈ôev√©st
            if not pd.api.types.is_numeric_dtype(y_pd.dtype):
                 # Pokus√≠ se p≈ôev√©st nap≈ô. True/False nebo 'Ano'/'Ne' na 0/1
                 y_pd, _ = pd.factorize(y_pd) # Vrac√≠ k√≥dy a unik√°tn√≠ hodnoty
                 if len(_) != 2: raise ValueError("Logistick√° regrese vy≈æaduje p≈ôesnƒõ 2 kategorie v Y.")
                 y_sm = pd.Series(y_pd, index=X_pd.index).astype(int) # Zajist√≠me int a zachov√°me index
                 logger.info(f"Y p≈ôevedeno na k√≥dy 0/1 pro Logit.")
            else:
                 y_sm = y_pd.astype(int) # Zajist√≠me integer typ

            X_sm_df = X_pd.astype(float)
            X_sm = sm.add_constant(X_sm_df, has_constant='raise')

            try:
                model = Logit(y_sm, X_sm)
                results = model.fit(method='newton') # Bƒõ≈æn√Ω solver pro Logit
                logger.info("Statsmodels Logit model fitted.")
            except PerfectSeparationError:
                 logger.error("Perfect separation detected during Logit fit.")
                 raise HTTPException(status_code=400, detail="Chyba: Nastala perfektn√≠ separace dat. Logistick√° regrese nem≈Ø≈æe b√Ωt spolehlivƒõ odhadnuta. Zkontrolujte, zda nƒõkter√° nez√°visl√° promƒõnn√° dokonale nepredikuje v√Ωsledek.")
            except Exception as fit_err:
                 logger.error(f"Error fitting Logit model: {fit_err}", exc_info=True)
                 raise HTTPException(status_code=500, detail=f"Chyba p≈ôi fitov√°n√≠ Logit modelu: {fit_err}")


            # Extrakce metrik a statistik (jako ƒç√≠sla)
            model_results["pseudo_r2"] = getattr(results, 'prsquared', None) # Pseudo R^2
            model_results["log_likelihood"] = getattr(results, 'llf', None)
            model_results["llr_p_value"] = getattr(results, 'llr_pvalue', None) # P-hodnota Likelihood Ratio testu
            model_results["n_observations"] = int(getattr(results, 'nobs', 0))
            model_results["intercept"] = results.params.get('const')
            model_results["r2"]=None; model_results["r2_adjusted"]=None; model_results["mse"]=None; model_results["rmse"]=None; model_results["f_statistic"]=None; model_results["f_pvalue"]=None;

            # V√Ωpoƒçet Accuracy (statsmodels ji p≈ô√≠mo ned√°v√°)
            y_pred_prob = results.predict(X_sm)
            y_pred_class = (y_pred_prob > 0.5).astype(int) # Klasifikace podle prahu 0.5
            model_results["accuracy"] = accuracy_score(y_sm, y_pred_class)

            # Extrakce koeficient≈Ø a statistik (jako ƒç√≠sla)
            coeffs_list = []
            params = results.params.drop('const', errors='ignore')
            p_values = results.pvalues.drop('const', errors='ignore')
            try: conf_int_df = results.conf_int() # CI pro log-odds
            except Exception: conf_int_df = pd.DataFrame(index=params.index, columns=[0, 1], data=np.nan)
            bse = results.bse.drop('const', errors='ignore') # Std. Error
            zvalues = results.tvalues.drop('const', errors='ignore') # Z-values (pojmenov√°no tvalues)

            for name in params.index:
                coeffs_list.append({
                    "name": name,
                    "coef": params.get(name),    # Log-odds
                    "stderr": bse.get(name),
                    "z_value": zvalues.get(name), # P≈ôejmenov√°no pro srozumitelnost
                    "p_value": p_values.get(name),
                    "ciLow": conf_int_df.loc[name, 0] if name in conf_int_df.index else None,
                    "ciHigh": conf_int_df.loc[name, 1] if name in conf_int_df.index else None,
                })
            model_results["coefficients"] = coeffs_list
            model_results["scatter_data"] = None # Scatter nen√≠ vhodn√Ω
            model_results["residuals"] = None    # Rezidua nejsou standardn√≠
            model_results["note"] = "V√Ωsledky z Logistick√© regrese (statsmodels Logit). Koeficienty jsou log-odds."

        # --- MULTINOMI√ÅLN√ç REGRESE pomoc√≠ STATSMODELS ---
        elif selected_method == "multinomial":
            logger.info("Using statsmodels.MNLogit for analysis.")
            # Y mus√≠ b√Ωt numerick√© kategorie (0, 1, 2...)
            y_codes, y_categories = pd.factorize(y_pd)
            y_mn = pd.Series(y_codes, index=X_pd.index).astype(int)
            logger.info(f"Y p≈ôevedeno na k√≥dy 0-{len(y_categories)-1} pro MNLogit. Kategorie: {list(y_categories)}")

            X_sm_df = X_pd.astype(float)
            X_sm = sm.add_constant(X_sm_df, has_constant='raise')

            try:
                model = MNLogit(y_mn, X_sm)
                # 'bfgs' nebo 'newton' jsou bƒõ≈æn√©, 'nm' m≈Ø≈æe b√Ωt pomalej≈°√≠
                results = model.fit(method='bfgs', maxiter=300) # Zv√Ω≈°en√≠ iterac√≠
                logger.info("Statsmodels MNLogit model fitted.")
            except Exception as fit_err:
                 logger.error(f"Error fitting MNLogit model: {fit_err}", exc_info=True)
                 # M≈Ø≈æe selhat kv≈Øli konvergenci, separaci atd.
                 raise HTTPException(status_code=500, detail=f"Chyba p≈ôi fitov√°n√≠ MNLogit modelu: {fit_err}")

            # Extrakce metrik a statistik
            model_results["pseudo_r2"] = getattr(results, 'prsquared', None)
            model_results["log_likelihood"] = getattr(results, 'llf', None)
            model_results["llr_p_value"] = getattr(results, 'llr_pvalue', None)
            model_results["n_observations"] = int(getattr(results, 'nobs', 0))
            # Intercept a koeficienty jsou slo≈æitƒõj≈°√≠
            model_results["intercept"] = None # MNLogit nem√° jeden intercept
            model_results["r2"]=None; model_results["r2_adjusted"]=None; model_results["mse"]=None; model_results["rmse"]=None; model_results["f_statistic"]=None; model_results["f_pvalue"]=None;

            # V√Ωpoƒçet Accuracy
            y_pred_prob = results.predict(X_sm) # Vrac√≠ pravdƒõpodobnosti pro ka≈ædou t≈ô√≠du
            y_pred_class_idx = np.argmax(y_pred_prob.values, axis=1) # Index t≈ô√≠dy s nejvy≈°≈°√≠ P
            model_results["accuracy"] = accuracy_score(y_mn, y_pred_class_idx)

            # Extrakce koeficient≈Ø (params je DataFrame: index=features, columns=classes)
            # A statistik (pvalues, bse, tvalues/zvalues jsou tak√© DataFrames)
            coeffs_list = []
            params_df = results.params # DataFrame (features+const x k-1 classes)
            pvalues_df = results.pvalues
            bse_df = results.bse
            zvalues_df = results.tvalues # Z-values

            # Referenƒçn√≠ t≈ô√≠da je prvn√≠ (index 0), ostatn√≠ jsou porovn√°ny v≈Øƒçi n√≠
            # Sloupce v params_df jsou n√°zvy ostatn√≠ch t≈ô√≠d (kategori√≠)
            feature_names = X_sm.columns.drop('const', errors='ignore') # N√°zvy X promƒõnn√Ωch

            for class_idx_str in params_df.columns: # Iterujeme p≈ôes c√≠lov√© t≈ô√≠dy (sloupce)
                try:
                    # Z√≠sk√°me p≈Øvodn√≠ n√°zev kategorie z indexu
                    class_idx = int(class_idx_str) # Sloupce jsou ƒçasto ƒç√≠sla 1, 2,...
                    class_name = y_categories[class_idx] # N√°zev kategorie
                except (ValueError, IndexError):
                     class_name = class_idx_str # Fallback na index jako string

                for feature_name in feature_names: # Iterujeme p≈ôes promƒõnn√© (≈ô√°dky)
                    if feature_name in params_df.index: # Jistota
                         coeffs_list.append({
                             "name": f"{feature_name} (t≈ô√≠da: {class_name})", # N√°zev s t≈ô√≠dou
                             "coef": params_df.loc[feature_name, class_idx_str],
                             "stderr": bse_df.loc[feature_name, class_idx_str] if feature_name in bse_df.index else None,
                             "z_value": zvalues_df.loc[feature_name, class_idx_str] if feature_name in zvalues_df.index else None,
                             "p_value": pvalues_df.loc[feature_name, class_idx_str] if feature_name in pvalues_df.index else None,
                             "ciLow": None, # CI pro MNLogit nejsou p≈ô√≠mo v summary
                             "ciHigh": None,
                         })

            model_results["coefficients"] = coeffs_list
            model_results["scatter_data"] = None
            model_results["residuals"] = None
            model_results["note"] = f"V√Ωsledky z Multinomi√°ln√≠ regrese (statsmodels MNLogit). Koeficienty jsou log-odds pro danou t≈ô√≠du vs. referenƒçn√≠ t≈ô√≠du '{y_categories[0]}'."


        # --- Ridge, Lasso, ElasticNet pomoc√≠ SKLEARN (z≈Øst√°v√° stejn√©) ---
        elif selected_method in ["ridge", "lasso", "elasticnet"]:
            logger.info(f"Using sklearn {selected_method.upper()} for analysis.")
            scaler = StandardScaler()
            # ---> ZMƒöNA: Pou≈æ√≠v√°me X_pd (DataFrame) pro select_dtypes <---
            X_numeric_df = X_pd.select_dtypes(include=np.number).astype(float)
            if X_numeric_df.shape[1] == 0: raise ValueError(f"≈Ω√°dn√© numerick√© X pro {selected_method}.")
            if X_numeric_df.shape[1] < X_pd.shape[1]: logger.warning(f"Nenumerick√© sloupce vypu≈°tƒõny pro {selected_method}.")

            X_scaled = scaler.fit_transform(X_numeric_df.values) # Fit na numpy array
            y_numeric = y_pd.astype(float).values # y jako numpy array

            alpha_val = 1.0
            if selected_method == "ridge": model = Ridge(alpha=alpha_val, random_state=42)
            elif selected_method == "lasso": model = Lasso(alpha=alpha_val, random_state=42)
            else: model = ElasticNet(alpha=alpha_val, random_state=42)

            model.fit(X_scaled, y_numeric)
            y_pred = model.predict(X_scaled)
            logger.info(f"Sklearn {selected_method.upper()} fitted.")

            # Extrakce v√Ωsledk≈Ø (bez detailn√≠ch statistik)
            model_results["intercept"] = float(model.intercept_)
            model_results["coefficients"] = [
                {"name": name, "coef": float(coef), "stderr": None, "t_value": None, "p_value": None, "ciLow": None, "ciHigh": None}
                for name, coef in zip(X_numeric_df.columns, model.coef_) # Pou≈æijeme sloupce z X_numeric_df
            ]
            model_results["r2"] = r2_score(y_numeric, y_pred)
            model_results["mse"] = mean_squared_error(y_numeric, y_pred)
            model_results["rmse"] = np.sqrt(model_results["mse"]) if model_results["mse"] >=0 else None
            model_results["n_observations"] = len(y_numeric)
            model_results["r2_adjusted"] = None; model_results["f_statistic"] = None; model_results["f_pvalue"]=None; model_results["accuracy"] = None

            # Data pro grafy
            residuals_val = y_numeric - y_pred
            if X_numeric_df.shape[1] == 1:
                 x_scatter = X_numeric_df.iloc[:, 0] # P≈Øvodn√≠ X (DataFrame)
                 if len(x_scatter) == len(y_numeric) == len(y_pred):
                      model_results["scatter_data"] = {"x": x_scatter.tolist(), "y_true": y_numeric.tolist(), "y_pred": y_pred.tolist()}
                 else: model_results["scatter_data"] = None
            else: model_results["scatter_data"] = None
            if len(y_pred) == len(residuals_val):
                 model_results["residuals"] = {"predicted": y_pred.tolist(), "residuals": residuals_val.tolist()}
            else: model_results["residuals"] = None

            model_results["note"] = f"V√Ωsledky z {selected_method.upper()} (sklearn). Koeficienty pro ≈°k√°lovan√° X."

        # --- Nezn√°m√° metoda ---
        else:
            raise NotImplementedError(f"Metoda '{selected_method}' nen√≠ implementov√°na.")


        # --- Fin√°ln√≠ ƒåi≈°tƒõn√≠ V√Ωsledk≈Ø pro JSON ---
        logger.info("Cleaning final results for JSON serialization...")
        final_results = robust_clean_nan_inf(model_results)

        logger.info("Regression analysis successful, returning results.")
        return final_results

    except (ValueError, NotImplementedError, MemoryError, HTTPException) as user_err:
        logger.error(f"Error during analysis ({selected_method}): {user_err}", exc_info=isinstance(user_err, ValueError))
        if isinstance(user_err, HTTPException): raise user_err
        raise HTTPException(status_code=400, detail=f"Chyba anal√Ωzy ({selected_method}): {str(user_err)}")
    except Exception as e:
        logger.exception(f"--- UNEXPECTED ERROR in Regression Analysis (Method: {selected_method}) ---")
        raise HTTPException(status_code=500, detail=f"Intern√≠ chyba serveru p≈ôi v√Ωpoƒçtu regrese ({selected_method}): {str(e)}")

# ... (importy jako d≈ô√≠ve: FastAPI, HTTPException, pandas, atd.)
# P≈ôidejte pot≈ôebn√© importy pro porovn√°n√≠ skupin
from scipy.stats import ttest_ind, ttest_rel, mannwhitneyu, wilcoxon, f_oneway, kruskal, levene # P≈ôid√°n levene


# ... (definice app, stored_datasets, funkce check_normality jako d≈ô√≠ve)
# ... (Endpoint /api/group_comparison z≈Øst√°v√°)


# --- Nov√© Pydantic modely pro AI interpretaci porovn√°n√≠ skupin ---

class TestResultInterpretation(BaseModel):
    """Zjednodu≈°en√° data o jednom testu pro AI"""
    numeric_variable: str
    test_name: str
    p_value: float
    is_significant: bool
    notes: Optional[str] = None # D≈Øvod v√Ωbƒõru testu

class GroupComparisonInterpretation(BaseModel):
    """Data o porovn√°n√≠ pro jednu skupinovou promƒõnnou"""
    group_variable: str
    tests_performed: List[TestResultInterpretation]

class GroupComparisonInterpretationRequest(BaseModel):
    analysis_type: str = "group_comparison"
    paired_analysis: bool
    comparisons: List[GroupComparisonInterpretation]


# --- Nov√Ω endpoint pro AI interpretaci porovn√°n√≠ skupin ---
@app.post("/api/interpret_group_comparison")
async def interpret_group_comparison(req: GroupComparisonInterpretationRequest):
    # --- System Prompt pro LLM ---
    system_prompt = (
        "Jsi AI asistent specializuj√≠c√≠ se na anal√Ωzu dat. U≈æivatel provedl porovn√°n√≠ skupin a poskytne ti v√Ωsledky.\n\n"
        "Tv√Ωm √∫kolem je interpretovat tyto v√Ωsledky v ƒçe≈°tinƒõ, jednoduch√Ωm a srozumiteln√Ωm jazykem.\n\n"
        f"Jednalo se o {'p√°rovou anal√Ωzu (porovn√°n√≠ dvou mƒõ≈ôen√≠ u stejn√Ωch subjekt≈Ø)' if req.paired_analysis else 'anal√Ωzu nez√°visl√Ωch skupin'}.\n\n"
        "Pro ka≈æd√Ω proveden√Ω test:\n"
        "- Uveƒè, kter√° ƒç√≠seln√° promƒõnn√° byla porovn√°v√°na mezi skupinami definovan√Ωmi kterou kategori√°ln√≠ promƒõnnou.\n"
        "- Zmi≈à pou≈æit√Ω test (nap≈ô. t-test, ANOVA, Mann-Whitney U, Wilcoxon, Kruskal-Wallis).\n"
        "- **Interpretuj p-hodnotu:**\n"
        "  - Pokud je p < 0.05, v√Ωsledek je **statisticky v√Ωznamn√Ω**. Popi≈° to jako 'Byl nalezen statisticky v√Ωznamn√Ω rozd√≠l v [ƒç√≠seln√° promƒõnn√°] mezi skupinami [kategori√°ln√≠ promƒõnn√°].' U p√°rov√©ho testu: 'Byl nalezen statisticky v√Ωznamn√Ω rozd√≠l mezi [ƒç√≠seln√° promƒõnn√° 1] a [ƒç√≠seln√° promƒõnn√° 2].'\n"
        "  - Pokud je p >= 0.05, v√Ωsledek **nen√≠ statisticky v√Ωznamn√Ω**. Popi≈° to jako 'Nebyl nalezen statisticky v√Ωznamn√Ω rozd√≠l...'.\n"
        "- M≈Ø≈æe≈° struƒçnƒõ zm√≠nit d≈Øvod v√Ωbƒõru testu, pokud je uveden v pozn√°mce (nap≈ô. kv≈Øli normalitƒõ dat, poƒçtu skupin, p√°rov√°n√≠).\n\n"
        "**Celkov√© shrnut√≠:**\n"
        "- Shrnout nejd≈Øle≈æitƒõj≈°√≠ (v√Ωznamn√©) rozd√≠ly, kter√© byly nalezeny.\n"
        "- Pokud nebyly nalezeny ≈æ√°dn√© v√Ωznamn√© rozd√≠ly, konstatuj to.\n\n"
        "**D≈Øle≈æit√© upozornƒõn√≠:**\n"
        "- P≈ôipome≈à, ≈æe statistick√° v√Ωznamnost (p < 0.05) neznamen√° automaticky velk√Ω nebo prakticky d≈Øle≈æit√Ω rozd√≠l (velikost efektu zde nen√≠ hodnocena).\n"
        "- U nez√°visl√Ωch test≈Ø (ANOVA, Kruskal-Wallis) v√Ωznamn√Ω v√Ωsledek ≈ô√≠k√°, ≈æe existuje rozd√≠l *nƒõkde* mezi skupinami, ale ne≈ô√≠k√° *mezi kter√Ωmi konkr√©tnƒõ* (k tomu by byly pot≈ôeba post-hoc testy).\n\n"
        "Pravidla:\n"
        "- Odpov√≠dej v ƒçe≈°tinƒõ.\n"
        "- Buƒè jasn√Ω a srozumiteln√Ω.\n"
        "- Zamƒõ≈ô se na interpretaci p-hodnoty v kontextu porovn√°n√≠ skupin.\n"
        "- Form√°tuj odpovƒõƒè pro dobrou ƒçitelnost (odstavce, body)."
    )

    # --- Sestaven√≠ User Promptu ---
    user_prompt_parts = [
        f"Provedl jsem anal√Ωzu typu '{req.analysis_type}' ({'p√°rov√°' if req.paired_analysis else 'nez√°visl√© skupiny'}).",
        "\nZde jsou v√Ωsledky jednotliv√Ωch porovn√°n√≠:"
    ]

    for comp in req.comparisons:
        user_prompt_parts.append(f"\nPorovn√°n√≠ podle skupinov√© promƒõnn√©: **{comp.group_variable}**")
        for test in comp.tests_performed:
            significance = '(v√Ωznamn√©)' if test.is_significant else '(nev√Ωznamn√©)'
            note_text = f" (Pozn.: {test.notes})" if test.notes else ""
            user_prompt_parts.append(
                f"- Promƒõnn√° '{test.numeric_variable}': Test='{test.test_name}', p-hodnota={test.p_value:.4f} {significance}{note_text}"
            )

    user_prompt_parts.append("\nPros√≠m, interpretuj tyto v√Ωsledky.")
    full_user_prompt = "\n".join(user_prompt_parts)

    # --- Vol√°n√≠ LLM API ---
    try:
        api_key = "OPENROUTER_API_KEY"
        if not api_key:
             raise HTTPException(status_code=500, detail="Chyb√≠ konfigurace API kl√≠ƒçe pro LLM.")

        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek/deepseek-chat-v3-0324:free", # Nebo jin√Ω model
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_user_prompt}
                ],
                "max_tokens": 800 # M≈Ø≈æe b√Ωt pot≈ôeba v√≠ce pro v√≠ce test≈Ø
            },
            timeout=90 # Del≈°√≠ timeout pro potenci√°lnƒõ v√≠ce vol√°n√≠
        )
        response.raise_for_status()

        llm_data = response.json()
        interpretation_text = llm_data.get("choices", [{}])[0].get("message", {}).get("content")

        if not interpretation_text:
             raise HTTPException(status_code=500, detail="AI nevr√°tila platnou interpretaci.")

        return {"interpretation": interpretation_text}

    except requests.exceptions.RequestException as req_err:
        print(f"Chyba komunikace s LLM API: {req_err}")
        raise HTTPException(status_code=503, detail=f"Chyba p≈ôi komunikaci s AI slu≈æbou: {req_err}")
    except Exception as e:
        print(f"Neoƒçek√°van√° chyba p≈ôi interpretaci porovn√°n√≠ skupin: {e}")
        # print(f"User prompt was: {full_user_prompt}") # Pro debugging
        raise HTTPException(status_code=500, detail=f"Intern√≠ chyba serveru p≈ôi generov√°n√≠ interpretace: {str(e)}")

# ... (zbytek va≈°√≠ FastAPI aplikace, vƒçetnƒõ check_normality, pokud ji pou≈æ√≠v√°te)
# Ujistƒõte se, ≈æe check_normality je definovan√° a dostupn√°

async def check_normality():
    # Implementace va≈°√≠ funkce check_normality zde...
    # Mƒõla by vr√°tit strukturu podobnou:
    # return {"results": [{"column": "col_name", "isNormal": True/False}, ...]}
    # Pokud ji nem√°te, budete muset upravit logiku v /api/group_comparison
    # nebo ji implementovat. Prozat√≠m vr√°t√≠m pr√°zdn√Ω placeholder.
    print("VAROV√ÅN√ç: Funkce check_normality nen√≠ plnƒõ implementov√°na v tomto p≈ô√≠kladu.")
    # Naƒçtƒõte data, proveƒète Shapiro-Wilk test pro ka≈æd√Ω ƒç√≠seln√Ω sloupec
    # a vra≈•te v√Ωsledky. P≈ô√≠klad:
    if "latest" not in stored_datasets: return {"results": []}
    df = pd.DataFrame(stored_datasets["latest"]["data"], columns=stored_datasets["latest"]["headers"])
    num_cols = [c["name"] for c in stored_datasets["latest"].get("column_types",{}).values() if c["type"] == "ƒå√≠seln√Ω"]
    normality_results = []
    for col in num_cols:
        try:
            series = pd.to_numeric(df[col], errors='coerce').dropna()
            if len(series) >= 3:
                stat, p_value = shapiro(series)
                normality_results.append({"column": col, "isNormal": bool(p_value > 0.05)})
            else:
                normality_results.append({"column": col, "isNormal": False}) # M√°lo dat pro test
        except Exception:
                normality_results.append({"column": col, "isNormal": False}) # Chyba p≈ôi zpracov√°n√≠
    return {"results": normality_results}




class CorrelationPairInterpretation(BaseModel):
    """Zjednodu≈°en√° data o p√°ru pro AI interpretaci"""
    var1: str
    var2: str
    correlation: float
    pValue: float
    significant: bool
    strength: str
class CorrelationInterpretationRequest(BaseModel):
    analysis_type: str = "correlation"
    method: str
    variables: List[str]
    correlation_pairs: List[CorrelationPairInterpretation]
    visualization_type: str # "matrix" nebo "scatterplot"




@app.post("/api/interpret_correlation")
async def interpret_correlation(req: CorrelationInterpretationRequest):
    # --- System Prompt pro LLM (interpretace korelace) ---
    system_prompt = (
        "Jsi AI asistent specializuj√≠c√≠ se na anal√Ωzu dat. U≈æivatel provedl korelaƒçn√≠ anal√Ωzu a poskytne ti jej√≠ v√Ωsledky.\n\n"
        "Tv√Ωm √∫kolem je interpretovat tyto v√Ωsledky v ƒçe≈°tinƒõ, jednoduch√Ωm a srozumiteln√Ωm jazykem pro nƒõkoho, kdo nemus√≠ b√Ωt expert na statistiku.\n\n"
        f"Pou≈æit√° metoda byla '{req.method}'. Struƒçnƒõ zmi≈à, pro jak√Ω typ vztahu je tato metoda vhodn√° (Pearson = line√°rn√≠, Spearman/Kendall = monot√≥nn√≠/po≈ôadov√Ω).\n\n"
        "Pro ka≈æd√Ω v√Ωznamn√Ω p√°r (significant = true):\n"
        "- Uveƒè promƒõnn√©.\n"
        "- Popi≈° s√≠lu a smƒõr vztahu (pou≈æij hodnotu 'correlation' a 'strength'). Nap≈ô. 'Siln√° pozitivn√≠ korelace (r=0.8) naznaƒçuje, ≈æe kdy≈æ roste X, roste i Y.' nebo 'St≈ôedn√≠ negativn√≠ korelace (r=-0.4) naznaƒçuje, ≈æe kdy≈æ roste X, Y m√° tendenci klesat.'\n"
        "- Zmi≈à, ≈æe vztah je statisticky v√Ωznamn√Ω (p < 0.05), co≈æ znamen√°, ≈æe je nepravdƒõpodobn√©, ≈æe by ≈°lo o n√°hodu.\n\n"
        "Pro nev√Ωznamn√© p√°ry (significant = false):\n"
        "- M≈Ø≈æe≈° je zm√≠nit souhrnnƒõ nebo vynechat, pokud jich je mnoho. Uveƒè, ≈æe mezi nimi nebyl nalezen statisticky v√Ωznamn√Ω vztah.\n\n"
        "Celkov√© shrnut√≠:\n"
        "- Pokud bylo analyzov√°no v√≠ce ne≈æ 2 promƒõnn√© (visualization_type = 'matrix'), shr≈à nejd≈Øle≈æitƒõj≈°√≠ (nejsilnƒõj≈°√≠ v√Ωznamn√©) nalezen√© vztahy.\n"
        "- Pokud byl jen jeden p√°r (visualization_type = 'scatterplot'), shr≈à v√Ωsledek pro tento p√°r.\n"
        "- **D≈Øle≈æit√©:** Zd≈Ørazni, ≈æe **korelace neznamen√° kauzalitu** (to, ≈æe dvƒõ vƒõci spolu souvis√≠, neznamen√°, ≈æe jedna zp≈Øsobuje druhou).\n\n"
        "Pravidla:\n"
        "- Odpov√≠dej v ƒçe≈°tinƒõ.\n"
        "- Buƒè struƒçn√Ω a vƒõcn√Ω, ale srozumiteln√Ω.\n"
        "- Nepou≈æ√≠vej p≈ô√≠li≈° technick√Ω ≈æargon, pokud to nen√≠ nutn√© (vysvƒõtli p-hodnotu jednodu≈°e).\n"
        "- Neuv√°dƒõj vzorce ani k√≥d.\n"
        "- Form√°tuj odpovƒõƒè do odstavc≈Ø nebo bod≈Ø pro lep≈°√≠ ƒçitelnost.\n"
        "- Zamƒõ≈ô se na interpretaci, ne na opakov√°n√≠ ƒç√≠seln√Ωch hodnot (kromƒõ nap≈ô.  pro ilustraci)."
    )

    # --- Sestaven√≠ User Promptu z dat od frontendu ---
    user_prompt_parts = [
        f"Provedl jsem korelaƒçn√≠ anal√Ωzu ('{req.analysis_type}') metodou '{req.method}' pro promƒõnn√©: {', '.join(req.variables)}.",
        f"Vizualizace byla typu: {'maticov√° (v√≠ce promƒõnn√Ωch)' if req.visualization_type == 'matrix' else 'bodov√Ω graf (dvƒõ promƒõnn√©)'}.",
        "\nZde jsou v√Ωsledky pro jednotliv√© p√°ry:"
    ]

    significant_pairs = [p for p in req.correlation_pairs if p.significant]
    non_significant_pairs = [p for p in req.correlation_pairs if not p.significant]

    if significant_pairs:
        user_prompt_parts.append("\nStatisticky v√Ωznamn√© vztahy (p < 0.05):")
        for pair in significant_pairs:
            user_prompt_parts.append(
                f"- {pair.var1} a {pair.var2}: Korelace r={pair.correlation:.3f} (S√≠la: {pair.strength}), p-hodnota={pair.pValue:.4f}"
            )
    else:
        user_prompt_parts.append("\nNebyly nalezeny ≈æ√°dn√© statisticky v√Ωznamn√© vztahy (p < 0.05).")

    if non_significant_pairs and len(significant_pairs) < len(req.correlation_pairs): # Zm√≠n√≠me jen pokud existuj√≠ a nejsou v≈°echny v√Ωznamn√©
         user_prompt_parts.append(f"\nMezi {len(non_significant_pairs)} dal≈°√≠mi p√°ry nebyl nalezen statisticky v√Ωznamn√Ω vztah (p >= 0.05).")


    user_prompt_parts.append("\nPros√≠m, interpretuj tyto v√Ωsledky.")
    full_user_prompt = "\n".join(user_prompt_parts)

    # --- Vol√°n√≠ LLM API (stejn√© jako u regrese) ---
    try:
        api_key = "OPENROUTER_API_KEY"
        if not api_key:
             raise HTTPException(status_code=500, detail="Chyb√≠ konfigurace API kl√≠ƒçe pro LLM.")

        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek/deepseek-chat-v3-0324:free", # Nebo jin√Ω model
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_user_prompt}
                ],
                "max_tokens": 600 # Mo≈æn√° pot≈ôeba v√≠ce pro v√≠ce p√°r≈Ø
            },
            timeout=60
        )
        response.raise_for_status() # Chyba pro 4xx/5xx

        llm_data = response.json()
        interpretation_text = llm_data.get("choices", [{}])[0].get("message", {}).get("content")

        if not interpretation_text:
             raise HTTPException(status_code=500, detail="AI nevr√°tila platnou interpretaci.")

        return {"interpretation": interpretation_text}

    except requests.exceptions.RequestException as req_err:
        print(f"Chyba komunikace s LLM API: {req_err}")
        raise HTTPException(status_code=503, detail=f"Chyba p≈ôi komunikaci s AI slu≈æbou: {req_err}")
    except Exception as e:
        print(f"Neoƒçek√°van√° chyba p≈ôi interpretaci korelace: {e}")
        # print(f"User prompt was: {full_user_prompt}") # Pro debugging
        raise HTTPException(status_code=500, detail=f"Intern√≠ chyba serveru p≈ôi generov√°n√≠ interpretace: {str(e)}")

class CorrelationRequest(BaseModel):
    columns: List[str]
    method: str  # 'auto', 'pearson', 'spearman', 'kendall'

@app.post("/api/correlation_analysis")
async def correlation_analysis(req: CorrelationRequest):
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    headers = stored_datasets["latest"]["headers"]
    data = stored_datasets["latest"]["data"]
    df = pd.DataFrame(data, columns=headers)

    selected_cols = req.columns
    method = req.method.lower()

    if len(selected_cols) < 2:
        raise HTTPException(status_code=400, detail="Je t≈ôeba vybrat alespo≈à dvƒõ promƒõnn√©.")

    try:
        subdf = df[selected_cols].apply(pd.to_numeric, errors='coerce').dropna()
        sample_size = len(subdf)

        used_method = method
        reason = ""

        if method == "auto":
            # Pokud nejsou informace o normalitƒõ, dopoƒç√≠tej je
            if "normality" not in stored_datasets["latest"]:
                normality = {}
                for col in selected_cols:
                    series = subdf[col].dropna()
                    if len(series) >= 3:
                        stat, p_value = shapiro(series)
                        normality[col] = bool(p_value > 0.05)
                    else:
                        normality[col] = False
                stored_datasets["latest"]["normality"] = normality
            else:
                normality = stored_datasets["latest"]["normality"]

            use_pearson = True
            for col in selected_cols:
                if not normality.get(col, False):
                    use_pearson = False
                    break

            reason = "Test 'auto': zvolena metoda na z√°kladƒõ normality." + (
                " V≈°echny promƒõnn√© jsou norm√°ln√≠ ‚Üí Pearson." if use_pearson else " Nƒõkter√© promƒõnn√© nejsou norm√°ln√≠ ‚Üí Spearman.")
            used_method = "pearson" if use_pearson else "spearman"

        if used_method not in ["pearson", "spearman", "kendall"]:
            raise HTTPException(status_code=400, detail="Neplatn√° metoda korelace.")

        n = len(selected_cols)
        matrix = [[0.0 for _ in range(n)] for _ in range(n)]
        p_values = [[0.0 for _ in range(n)] for _ in range(n)]
        results = []

        def interpret_strength(r):
            abs_r = abs(r)
            if abs_r < 0.1:
                return "≈Ω√°dn√°/slab√°"
            elif abs_r < 0.3:
                return "Slab√°"
            elif abs_r < 0.5:
                return "St≈ôedn√≠"
            elif abs_r < 0.7:
                return "Siln√°"
            else:
                return "Velmi siln√°"

        def fisher_confidence_interval(r, n, alpha=0.05):
            if n < 4 or abs(r) >= 1.0:
                return (None, None)
            z = 0.5 * math.log((1 + r) / (1 - r))
            se = 1 / math.sqrt(n - 3)
            z_crit = norm.ppf(1 - alpha / 2)
            z_low = z - z_crit * se
            z_high = z + z_crit * se
            r_low = (math.exp(2 * z_low) - 1) / (math.exp(2 * z_low) + 1)
            r_high = (math.exp(2 * z_high) - 1) / (math.exp(2 * z_high) + 1)
            return (r_low, r_high)

        for i in range(n):
            for j in range(n):
                if i == j:
                    matrix[i][j] = 1.0
                    p_values[i][j] = 0.0
                elif j < i:
                    matrix[i][j] = matrix[j][i]
                    p_values[i][j] = p_values[j][i]
                else:
                    x = subdf[selected_cols[i]]
                    y = subdf[selected_cols[j]]

                    if used_method == "pearson":
                        corr, p = pearsonr(x, y)
                    elif used_method == "spearman":
                        corr, p = spearmanr(x, y)
                    elif used_method == "kendall":
                        corr, p = kendalltau(x, y)

                    matrix[i][j] = corr
                    p_values[i][j] = p

                    r2 = corr ** 2
                    ci_low, ci_high = fisher_confidence_interval(corr, sample_size)

                    results.append({
                        "var1": selected_cols[i],
                        "var2": selected_cols[j],
                        "correlation": float(corr),
                        "pValue": float(p),
                        "rSquared": float(r2),
                        "ciLow": ci_low,
                        "ciHigh": ci_high,
                        "strength": interpret_strength(corr),
                        "significant": bool(p < 0.05)
                    })

        scatter_data = None
        if len(selected_cols) == 2:
            scatter_data = {
                "x": subdf[selected_cols[0]].tolist(),
                "y": subdf[selected_cols[1]].tolist(),
                "xLabel": selected_cols[0],
                "yLabel": selected_cols[1]
            }

        return {
            "columns": selected_cols,
            "matrix": matrix,
            "pValues": p_values,
            "method": used_method,
            "reason": reason,
            "results": results,
            "scatterData": scatter_data
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chyba p≈ôi v√Ωpoƒçtu korelace: {str(e)}")


@app.options("/api/analyze")
async def options_analyze():
    return {"message": "CORS preflight OK"}

class DataRequest(BaseModel):
    headers: List[str]
    data: List[List[Optional[Union[str, float, int]]]]
# üîπ OPRAVA: T≈ô√≠da `DataRequest` byla zdvojena, upraveno na spr√°vnou verzi

# Glob√°ln√≠ cache pro ulo≈æen√° data
stored_datasets = {}


@app.post("/api/store_data")
async def store_data(request: Request):
    try:
        body = await request.json()
        headers = body.get("headers")
        data = body.get("data")

        if not headers or not data:
            raise HTTPException(status_code=400, detail="Missing headers or data")

        stored_datasets["latest"] = {
            "headers": headers,
            "data": data
        }

        return {"status": "ok"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class PromptRequest(BaseModel):
    prompt: str




class ClusterRequest(BaseModel):
    columns: List[str]
    algorithm: Literal["auto", "kmeans", "dbscan", "hierarchical"] = "auto"
    distance: Literal["auto", "euclidean", "manhattan", "cosine"] = "auto"
    num_clusters: Optional[int] = None
    standardize: bool = True

@app.post("/api/cluster_analysis")
async def cluster_analysis(req: ClusterRequest):
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    headers = stored_datasets["latest"]["headers"]
    data = stored_datasets["latest"]["data"]
    df = pd.DataFrame(data, columns=headers)

    if len(req.columns) < 2:
        raise HTTPException(status_code=400, detail="Je t≈ôeba vybrat alespo≈à dvƒõ promƒõnn√©.")

    df_subset = df[req.columns].apply(pd.to_numeric, errors="coerce").dropna()
    if df_subset.empty:
        raise HTTPException(status_code=400, detail="Vybran√© promƒõnn√© neobsahuj√≠ ≈æ√°dn√° validn√≠ data.")

    if req.standardize:
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(df_subset)
    else:
        data_scaled = df_subset.values

    n_samples, n_features = data_scaled.shape
    used_algorithm = req.algorithm
    used_distance = req.distance if req.distance != "auto" else "euclidean"
    used_clusters = req.num_clusters
    reason = ""

    # Automatick√° detekce algoritmu
    if req.algorithm == "auto":
        if n_samples < 500:
            used_algorithm = "hierarchical"
            reason = "Dataset je mal√Ω ‚Üí pou≈æita hierarchick√° shlukov√° anal√Ωza."
        elif n_features > 20:
            used_algorithm = "kmeans"
            reason = "Dataset m√° mnoho promƒõnn√Ωch ‚Üí pou≈æita metoda KMeans s mo≈ænost√≠ PCA."
        else:
            used_algorithm = "kmeans"
            reason = "Standardn√≠ podm√≠nky ‚Üí pou≈æita metoda KMeans."

    # Shlukov√°n√≠
    if used_algorithm == "kmeans":
        if req.num_clusters is None:
            sil_scores = []
            best_k = 2
            for k in range(2, min(10, n_samples)):
                model = KMeans(n_clusters=k, random_state=42)
                labels = model.fit_predict(data_scaled)
                score = silhouette_score(data_scaled, labels)
                sil_scores.append((k, score))
            best_k = max(sil_scores, key=lambda x: x[1])[0]
            used_clusters = best_k
            reason += f" Automaticky urƒçeno {best_k} shluk≈Ø pomoc√≠ silhouette sk√≥re."
        model = KMeans(n_clusters=used_clusters, random_state=42)

    elif used_algorithm == "hierarchical":
        used_clusters = req.num_clusters or 3
        if used_distance != "euclidean":
            model = AgglomerativeClustering(n_clusters=used_clusters, affinity=used_distance, linkage="average")
            reason += f" Pou≈æita metrika '{used_distance}' s linkage='average'."
        else:
            model = AgglomerativeClustering(n_clusters=used_clusters, linkage="ward")
            reason += " Pou≈æita metrika 'euclidean' s linkage='ward'."

    elif used_algorithm == "dbscan":
        model = DBSCAN(metric=used_distance)
    else:
        raise HTTPException(status_code=400, detail="Nezn√°m√Ω algoritmus.")

    labels = model.fit_predict(data_scaled)

    silhouette = None
    if used_algorithm in ["kmeans", "hierarchical"] and len(set(labels)) > 1:
        silhouette = silhouette_score(data_scaled, labels)

    df_result = df_subset.copy()
    df_result["Cluster"] = labels

    summary_df = df_result.groupby("Cluster").agg(['mean', 'std', 'min', 'max']).round(3)
    summary_dict = summary_df.to_dict(orient="index")

    summary = {}
    for cluster, stats_per_cluster in summary_dict.items():
        flat_stats = {}
        for key, stat_value in stats_per_cluster.items():
            if isinstance(key, tuple) and len(key) == 2:
                col, stat = key
                flat_stats[f"{col} ({stat})"] = float(stat_value)
            else:
                flat_stats[str(key)] = float(stat_value)
        summary[int(cluster)] = flat_stats

    # PCA pro vizualizaci
    pca = PCA(n_components=2)
    pca_components = pca.fit_transform(data_scaled)

    # 2D projekce
    pca_2d = {
        "x": pca_components[:, 0].tolist(),
        "y": pca_components[:, 1].tolist(),
        "labels": labels.tolist()
    }

    # Biplot - smƒõry promƒõnn√Ωch
    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
    pca_vectors = []
    for name, vec in zip(req.columns, loadings):
        pca_vectors.append({
            "name": name,
            "x": float(vec[0]),
            "y": float(vec[1])
        })

    # Variance tabulka (volitelnƒõ pro dal≈°√≠ v√Ωstupy)
    pca_variance_table = []
    cumulative = 0
    for i, (eig, ratio) in enumerate(zip(pca.explained_variance_, pca.explained_variance_ratio_)):
        cumulative += ratio
        pca_variance_table.append({
            "Component": f"PC{i + 1}",
            "Eigenvalue": round(eig, 4),
            "% Variance": round(ratio * 100, 2),
            "Cumulative %": round(cumulative * 100, 2)
        })

    return {
        "method": used_algorithm,
        "reason": reason,
        "clusters": used_clusters,
        "distance_metric": used_distance,
        "silhouette_score": silhouette,
        "summary": summary,
        "labels": labels.tolist(),
        "columns": req.columns,
        "pca_2d": pca_2d,
        "pca_components": pca_vectors,
        "pca_variance_table": pca_variance_table
    }

class FactorAnalysisInterpretationRequest(BaseModel):
    analysis_type: str = "factor_analysis" # Keep consistent pattern
    columns_used: List[str]
    n_factors_extracted: int
    rotation_used: str
    standardized: bool
    # dropped_rows: int # Less critical for interpretation itself
    kmo_model: Optional[float] = None # Use Optional for safety
    bartlett_p_value: Optional[float] = None
    total_variance_explained_pct: Optional[float] = None

# --- Helper Functions ---
# ... (Keep safe_float function) ...

# --- API Endpoints ---
# ... (Keep ALL your existing endpoints like /api/classification_analysis, /api/factor_analysis, etc.) ...


# --- NEW Endpoint for AI Interpretation of Factor Analysis ---
@app.post("/api/interpret_factor_analysis")
async def interpret_factor_analysis(req: FactorAnalysisInterpretationRequest):
    logger.info(f"Received factor analysis interpretation request for columns: {req.columns_used}")

    # --- System Prompt for LLM ---
    system_prompt = (
        "Jsi AI asistent specializuj√≠c√≠ se na anal√Ωzu dat. U≈æivatel provedl faktorovou anal√Ωzu (FA) a poskytne ti jej√≠ kl√≠ƒçov√© v√Ωsledky.\n\n"
        "Tv√Ωm √∫kolem je interpretovat tyto v√Ωsledky v ƒçe≈°tinƒõ, jednoduch√Ωm a srozumiteln√Ωm jazykem pro nƒõkoho bez hlubok√Ωch znalost√≠ statistiky.\n\n"
        "**Co je Faktorov√° Anal√Ωza (struƒçnƒõ):**\n"
        "FA se sna≈æ√≠ naj√≠t skryt√© (latentn√≠) 'faktory', kter√© vysvƒõtluj√≠ vz√°jemn√© korelace mezi p≈Øvodn√≠mi pozorovan√Ωmi promƒõnn√Ωmi. C√≠lem je zjednodu≈°it data a identifikovat z√°kladn√≠ struktury.\n\n"
        f"**V√Ωsledky t√©to anal√Ωzy:**\n"
        f"- Bylo extrahov√°no **{req.n_factors_extracted}** faktor≈Ø z promƒõnn√Ωch: {', '.join(req.columns_used)}.\n"
        f"- Pou≈æit√° metoda rotace byla: **{req.rotation_used}**. (Rotace pom√°h√° l√©pe interpretovat faktory; '{req.rotation_used}' je bƒõ≈æn√° volba).\n"
        f"- Data byla p≈ôed anal√Ωzou {'standardizov√°na (p≈ôevedena na stejn√© mƒõ≈ô√≠tko)' if req.standardized else 'pou≈æita bez standardizace'}.\n\n"
        "**Hodnocen√≠ vhodnosti dat a modelu:**\n"
        f"- **KMO test (Kaiser-Meyer-Olkin):** {f'Hodnota {req.kmo_model:.3f}.' if req.kmo_model is not None else 'N/A.'} "
        f"{interpret_kmo(req.kmo_model) if req.kmo_model is not None else ''} (Hodnoty nad 0.6 jsou obecnƒõ pova≈æov√°ny za p≈ôijateln√©, vy≈°≈°√≠ jsou lep≈°√≠. Ukazuje, zda data maj√≠ dostatek spoleƒçn√© variance pro FA.)\n"
        f"- **Bartlett≈Øv test sf√©rickosti:** {f'p-hodnota = {req.bartlett_p_value:.4g}.' if req.bartlett_p_value is not None else 'N/A.'} "
        f"{'Tento v√Ωsledek je statisticky v√Ωznamn√Ω (p < 0.05), co≈æ naznaƒçuje, ≈æe mezi promƒõnn√Ωmi existuj√≠ korelace a data JSOU vhodn√° pro FA.' if req.bartlett_p_value is not None and req.bartlett_p_value < 0.05 else ('Tento v√Ωsledek NEN√ç statisticky v√Ωznamn√Ω (p >= 0.05), co≈æ znamen√°, ≈æe data NEMUS√ç b√Ωt vhodn√° pro FA (promƒõnn√© spolu dostateƒçnƒõ nekoreluj√≠).' if req.bartlett_p_value is not None else '')}\n"
        f"- **Celkov√° vysvƒõtlen√° variance:** {f'Nalezen√© faktory spoleƒçnƒõ vysvƒõtluj√≠ **{req.total_variance_explained_pct:.1f}%** celkov√© variability p≈Øvodn√≠ch promƒõnn√Ωch.' if req.total_variance_explained_pct is not None else 'Informace o celkov√© vysvƒõtlen√© varianci nen√≠ k dispozici.'} (Vy≈°≈°√≠ procento znamen√°, ≈æe faktory l√©pe zachycuj√≠ informace z p≈Øvodn√≠ch dat. ƒåasto se hled√° hodnota alespo≈à 50-60 %.)\n\n"
        "**Dal≈°√≠ informace (z tabulek, kter√© u≈æivatel vid√≠):**\n"
        "- Tabulka **Faktorov√© z√°tƒõ≈æe (Loadings)** ukazuje, jak silnƒõ ka≈æd√° p≈Øvodn√≠ promƒõnn√° koreluje s ka≈æd√Ωm extrahovan√Ωm faktorem. Vysok√© z√°tƒõ≈æe (obvykle > 0.4 nebo 0.5) pom√°haj√≠ pochopit, co dan√Ω faktor reprezentuje.\n"
        "- **Komunality** ukazuj√≠, jak√Ω pod√≠l variance *ka≈æd√© jednotliv√©* promƒõnn√© je vysvƒõtlen v≈°emi nalezen√Ωmi faktory dohromady.\n\n"
        "**Celkov√© shrnut√≠:**\n"
        "- Zhodno≈•, zda se na z√°kladƒõ KMO, Bartlettova testu a vysvƒõtlen√© variance zd√° anal√Ωza smyslupln√° a model u≈æiteƒçn√Ω.\n"
        f"- Zd≈Ørazni, ≈æe pojmenov√°n√≠ a interpretace v√Ωznamu jednotliv√Ωch {req.n_factors_extracted} faktor≈Ø vy≈æaduje pod√≠vat se na faktorov√© z√°tƒõ≈æe (kter√© promƒõnn√© maj√≠ u dan√©ho faktoru vysokou hodnotu) a zapojit znalost oboru.\n\n"
        "Pravidla:\n"
        "- Odpov√≠dej v ƒçe≈°tinƒõ.\n"
        "- Buƒè jasn√Ω, struƒçn√Ω a srozumiteln√Ω pro laika.\n"
        "- Vysvƒõtli v√Ωznam kl√≠ƒçov√Ωch ƒç√≠sel jednodu≈°e.\n"
        "- Neuv√°dƒõj vzorce ani k√≥d.\n"
        "- Form√°tuj odpovƒõƒè pro dobrou ƒçitelnost (odstavce, tuƒçn√© p√≠smo)."
    )

    # --- Sestaven√≠ User Promptu ---
    # V tomto p≈ô√≠padƒõ je vƒõt≈°ina informac√≠ u≈æ v system promptu,
    # user prompt m≈Ø≈æe b√Ωt jednoduch√Ω nebo zopakovat kl√≠ƒçov√© metriky pro kontext.
    user_prompt_parts = [
        f"Provedl jsem faktorovou anal√Ωzu s n√°sleduj√≠c√≠mi v√Ωsledky:",
        f"- Poƒçet extrahovan√Ωch faktor≈Ø: {req.n_factors_extracted}",
        f"- Pou≈æit√° rotace: {req.rotation_used}",
        f"- KMO: {req.kmo_model:.3f}" if req.kmo_model is not None else "- KMO: N/A",
        f"- Bartlett p-value: {req.bartlett_p_value:.4g}" if req.bartlett_p_value is not None else "- Bartlett p-value: N/A",
        f"- Celkov√° vysvƒõtlen√° variance: {req.total_variance_explained_pct:.1f}%" if req.total_variance_explained_pct is not None else "- Celkov√° vysvƒõtlen√° variance: N/A",
        "\nPros√≠m, interpretuj tyto v√Ωsledky."
    ]
    full_user_prompt = "\n".join(user_prompt_parts)

    # --- Vol√°n√≠ LLM API (pou≈æijte va≈°i existuj√≠c√≠ logiku/kl√≠ƒç) ---
    try:

        api_key = "OPENROUTER_API_KEY"
        if not api_key:
             raise HTTPException(status_code=500, detail="Chyb√≠ konfigurace API kl√≠ƒçe pro LLM.")

        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions", # V√°≈° LLM endpoint
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek/deepseek-chat-v3-0324:free", # V√°≈° model
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_user_prompt}
                ],
                "max_tokens": 700 # M≈Ø≈æe b√Ωt pot≈ôeba v√≠ce pro FA
            },
            timeout=90 # Del≈°√≠ timeout pro komplexnƒõj≈°√≠ interpretaci
        )
        response.raise_for_status()

        llm_data = response.json()
        interpretation_text = llm_data.get("choices", [{}])[0].get("message", {}).get("content")

        if not interpretation_text:
             raise HTTPException(status_code=500, detail="AI nevr√°tila platnou interpretaci.")

        logger.info("AI interpretace pro FA √∫spƒõ≈°nƒõ vygenerov√°na.")
        return {"interpretation": interpretation_text}

    except requests.exceptions.RequestException as req_err:
        logger.error(f"Chyba komunikace s LLM API pro FA interpretaci: {req_err}", exc_info=True)
        raise HTTPException(status_code=503, detail=f"Chyba p≈ôi komunikaci s AI slu≈æbou: {req_err}")
    except Exception as e:
        logger.error(f"Neoƒçek√°van√° chyba p≈ôi interpretaci FA: {e}", exc_info=True)
        # print(f"System prompt was: {system_prompt}") # Pro debugging
        # print(f"User prompt was: {full_user_prompt}") # Pro debugging
        raise HTTPException(status_code=500, detail=f"Intern√≠ chyba serveru p≈ôi generov√°n√≠ interpretace FA: {str(e)}")


# --- Helper function for KMO interpretation (add this somewhere accessible) ---
def interpret_kmo(kmo_value: Optional[float]) -> str:
    if kmo_value is None:
        return ""
    if kmo_value < 0.5: return "Tato hodnota je nep≈ôijateln√°."
    if kmo_value < 0.6: return "Tato hodnota je mizern√°."
    if kmo_value < 0.7: return "Tato hodnota je slab√°."
    if kmo_value < 0.8: return "Tato hodnota je st≈ôedn√≠."
    if kmo_value < 0.9: return "Tato hodnota je dobr√°."
    return "Tato hodnota je vynikaj√≠c√≠."
class ClusterInterpretationRequest(BaseModel):
    analysis_type: str = "clustering"
    algorithm_used: str
    distance_metric: str
    number_of_clusters_found: Optional[int] = None # M≈Ø≈æe b√Ωt null pro DBSCAN
    silhouette_score: Optional[float] = None # M≈Ø≈æe b√Ωt null
    columns_used: List[str]
    # M≈Ø≈æeme p≈ôidat dal≈°√≠ info, nap≈ô. zda byla pou≈æita standardizace

# --- Nov√Ω endpoint pro AI interpretaci shlukov√°n√≠ ---
@app.post("/api/interpret_clustering")
async def interpret_clustering(req: ClusterInterpretationRequest):
    # --- System Prompt pro LLM ---
    system_prompt = (
        "Jsi AI asistent specializuj√≠c√≠ se na anal√Ωzu dat. U≈æivatel provedl shlukovou anal√Ωzu a poskytne ti jej√≠ kl√≠ƒçov√© v√Ωsledky.\n\n"
        "Tv√Ωm √∫kolem je interpretovat tyto v√Ωsledky v ƒçe≈°tinƒõ, jednoduch√Ωm a srozumiteln√Ωm jazykem.\n\n"
        f"Pou≈æit√Ω algoritmus byl: **{req.algorithm_used}** s metrikou vzd√°lenosti '{req.distance_metric}'. Anal√Ωza byla provedena na promƒõnn√Ωch: {', '.join(req.columns_used)}.\n\n"
        "**Interpretace v√Ωsledk≈Ø:**\n"
        f"- **Poƒçet nalezen√Ωch shluk≈Ø:** {'Bylo nalezeno ' + str(req.number_of_clusters_found) + ' shluk≈Ø.' if req.number_of_clusters_found is not None else 'Algoritmus DBSCAN nalezl shluky automaticky (poƒçet nen√≠ pevnƒõ dan√Ω).'} Pokud DBSCAN na≈°el body oznaƒçen√© jako ≈°um (-1), zmi≈à to - jsou to body, kter√© nezapadaj√≠ do ≈æ√°dn√©ho hust√©ho shluku.\n"
        f"- **Silhouette Score:** {'Silhouette sk√≥re bylo ' + f'{req.silhouette_score:.3f}' + '.' if req.silhouette_score is not None else 'Silhouette sk√≥re nebylo pro tento algoritmus (DBSCAN) relevantn√≠ nebo nebylo mo≈æn√© spoƒç√≠tat (nap≈ô. jen 1 shluk).'} "
        "Toto sk√≥re mƒõ≈ô√≠, jak dob≈ôe jsou body oddƒõleny mezi shluky a jak jsou si podobn√© body uvnit≈ô jednoho shluku. Hodnoty bl√≠zko +1 znamenaj√≠ dob≈ôe definovan√© shluky. Hodnoty kolem 0 znamenaj√≠ p≈ôekr√Ωvaj√≠c√≠ se shluky. Hodnoty bl√≠zko -1 znamenaj√≠, ≈æe body mohly b√Ωt p≈ôi≈ôazeny do ≈°patn√Ωch shluk≈Ø.\n\n"
        "**Celkov√© zhodnocen√≠:**\n"
        "- Na z√°kladƒõ poƒçtu shluk≈Ø a Silhouette sk√≥re (pokud je k dispozici) zhodno≈• kvalitu shlukov√°n√≠. Nap≈ô. 'Model na≈°el X dob≈ôe oddƒõlen√Ωch shluk≈Ø (vysok√© Silhouette sk√≥re).' nebo 'Nalezen√© shluky se zdaj√≠ b√Ωt p≈ôekr√Ωvaj√≠c√≠ (n√≠zk√©/nulov√© Silhouette sk√≥re).' nebo 'DBSCAN identifikoval nƒõkolik hust√Ωch oblast√≠ a mo≈æn√Ω ≈°um.'\n"
        "- Zm√≠nit, ≈æe vizualizace pomoc√≠ PCA (pokud byla zobrazena) pom√°h√° vidƒõt strukturu shluk≈Ø ve 2D, i kdy≈æ p≈Øvodn√≠ data mƒõla v√≠ce dimenz√≠.\n"
        "- P≈ôipome≈à, ≈æe interpretace *v√Ωznamu* jednotliv√Ωch shluk≈Ø (co charakterizuje shluk 1 vs. shluk 2) vy≈æaduje dal≈°√≠ anal√Ωzu pr≈Ømƒõr≈Ø/medi√°n≈Ø promƒõnn√Ωch v jednotliv√Ωch shluc√≠ch (co≈æ u≈æivatel vid√≠ v souhrnn√© tabulce).\n\n"
        "Pravidla:\n"
        "- Odpov√≠dej v ƒçe≈°tinƒõ.\n"
        "- Buƒè srozumiteln√Ω.\n"
        "- Vysvƒõtli v√Ωznam Silhouette sk√≥re jednodu≈°e.\n"
        "- Neuv√°dƒõj vzorce ani k√≥d.\n"
        "- Form√°tuj odpovƒõƒè pro dobrou ƒçitelnost."
    )

    # --- Sestaven√≠ User Promptu ---
    user_prompt_parts = [
        f"Provedl jsem shlukovou anal√Ωzu ('{req.analysis_type}') algoritmem '{req.algorithm_used}' s metrikou '{req.distance_metric}'.",
        f"Anal√Ωza probƒõhla na sloupc√≠ch: {', '.join(req.columns_used)}.",
    ]
    if req.number_of_clusters_found is not None:
        user_prompt_parts.append(f"Bylo nalezeno {req.number_of_clusters_found} shluk≈Ø.")
    else:
         user_prompt_parts.append("Poƒçet shluk≈Ø byl urƒçen automaticky (DBSCAN).")

    if req.silhouette_score is not None:
         user_prompt_parts.append(f"Silhouette sk√≥re: {req.silhouette_score:.4f}")
    else:
         user_prompt_parts.append("Silhouette sk√≥re nebylo relevantn√≠ nebo spoƒç√≠t√°no.")

    user_prompt_parts.append("\nPros√≠m, interpretuj tyto v√Ωsledky.")
    full_user_prompt = "\n".join(user_prompt_parts)

    # --- Vol√°n√≠ LLM API ---
    try:
        api_key ="OPENROUTER_API_KEY"
        if not api_key:
             raise HTTPException(status_code=500, detail="Chyb√≠ konfigurace API kl√≠ƒçe pro LLM.")

        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek/deepseek-chat-v3-0324:free",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_user_prompt}
                ],
                "max_tokens": 600
            },
            timeout=60
        )
        response.raise_for_status()

        llm_data = response.json()
        interpretation_text = llm_data.get("choices", [{}])[0].get("message", {}).get("content")

        if not interpretation_text:
             raise HTTPException(status_code=500, detail="AI nevr√°tila platnou interpretaci.")

        return {"interpretation": interpretation_text}

    except requests.exceptions.RequestException as req_err:
        logger.error(f"Chyba komunikace s LLM API: {req_err}")
        raise HTTPException(status_code=503, detail=f"Chyba p≈ôi komunikaci s AI slu≈æbou: {req_err}")
    except Exception as e:
        logger.error(f"Neoƒçek√°van√° chyba p≈ôi interpretaci shlukov√°n√≠: {e}", exc_info=True)
        # print(f"User prompt was: {full_user_prompt}") # Pro debugging
        raise HTTPException(status_code=500, detail=f"Intern√≠ chyba serveru p≈ôi generov√°n√≠ interpretace: {str(e)}")


@app.post("/api/ai_suggest_analysis")
async def ai_suggest_analysis(req: PromptRequest):
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="No stored data available.")

    headers = stored_datasets["latest"]["headers"]
    columns_text = ", ".join(headers)
    full_prompt = (
        f"I have a dataset with the following columns: {columns_text}.\n\n"
        f"My question is: {req.prompt}"
    )

    system_prompt = (
        "You are a helpful assistant for data analysis. The user will describe what they want to find out from their dataset.\n\n"
        "Your task is to classify the request into one of the following types of analysis:\n\n"
        "1. Porovn√°n√≠ skupin\n"
        "2. Vztah mezi promƒõnn√Ωmi\n"
        "3. Klasifikace\n"
        "4. Shlukov√° anal√Ωza\n"
        "5. Faktorov√° anal√Ωza\n\n"
        "Rules:\n"
        "The response will be in Czech.\n"
        "- Respond with **only one** of the above categories.\n"
        "- Include a **short explanation** (1‚Äì3 sentences) why this analysis is appropriate.\n"
        "- If you are not sure what type of analysis to recommend, ask the user to clarify their problem.\n\n"
        "Forbidden:\n"
        "- Do not mention statistical tests.\n"
        "- Do not include code or formulas.\n"
        "- Do not guess multiple types ‚Äî pick only one or ask for clarification."
    )

    try:
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": "Bearer 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",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek/deepseek-chat-v3-0324:free",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_prompt}
                ],
                "max_tokens": 300
            },
            timeout=60
        )

        return response.json()

    except Exception as e:
        return {"error": str(e)}

class CoefficientInterpretation(BaseModel):
    name: str
    value: Optional[float] = None # Pos√≠l√°me jen hodnotu pro jednoduchost

class ResultsSummaryInterpretation(BaseModel):
    r2: Optional[float] = None
    r2_adjusted: Optional[float] = None
    rmse: Optional[float] = None
    accuracy: Optional[float] = None
    overfitting_detected: Optional[bool] = None

class InterpretationRequest(BaseModel):
    analysis_type: str = "regression"
    method: str
    dependent_variable: str
    independent_variables: List[str]
    results_summary: ResultsSummaryInterpretation
    coefficients: List[CoefficientInterpretation]
    intercept: Optional[float] = None

# --- Nov√Ω endpoint pro AI interpretaci ---
@app.post("/api/interpret_regression")
async def interpret_regression(req: InterpretationRequest):
    # --- System Prompt pro LLM ---
    system_prompt = (
        "Jsi AI asistent specializuj√≠c√≠ se na anal√Ωzu dat. U≈æivatel provedl regresn√≠ anal√Ωzu a poskytne ti jej√≠ v√Ωsledky.\n\n"
        "Tv√Ωm √∫kolem je interpretovat tyto v√Ωsledky v ƒçe≈°tinƒõ, jednoduch√Ωm a srozumiteln√Ωm jazykem pro nƒõkoho, kdo nemus√≠ b√Ωt expert na statistiku.\n\n"
        "Zamƒõ≈ô se na:\n"
        "1.  **Celkovou kvalitu modelu:** Vysvƒõtli, co znamenaj√≠ kl√≠ƒçov√© metriky (R-squared pro regresi, Accuracy pro klasifikaci) a jak dob≈ôe model vysvƒõtluje data nebo predikuje v√Ωsledek.\n"
        "2.  **Vliv nez√°visl√Ωch promƒõnn√Ωch:** Popi≈°, kter√© promƒõnn√© se zdaj√≠ b√Ωt nejd≈Øle≈æitƒõj≈°√≠ (na z√°kladƒõ koeficient≈Ø) a jak√Ω maj√≠ vliv (pozitivn√≠/negativn√≠) na z√°vislou promƒõnnou. Zmi≈à i intercept (pr≈Øseƒç√≠k), pokud m√° smysluplnou interpretaci v kontextu.\n"
        "3.  **Potenci√°ln√≠ probl√©my:** Pokud byla detekov√°na zn√°mka overfittingu, upozorni na to a struƒçnƒõ vysvƒõtli, co to znamen√°.\n"
        "4.  **Pou≈æitou metodu:** Struƒçnƒõ zmi≈à pou≈æitou metodu a proƒç byla pravdƒõpodobnƒõ vhodn√° (nap≈ô. line√°rn√≠ regrese pro ƒç√≠seln√Ω v√Ωstup, logistick√° pro bin√°rn√≠).\n\n"
        "Pravidla:\n"
        "- Odpov√≠dej v ƒçe≈°tinƒõ.\n"
        "- Buƒè struƒçn√Ω a vƒõcn√Ω, ale srozumiteln√Ω.\n"
        "- Nepou≈æ√≠vej p≈ô√≠li≈° technick√Ω ≈æargon, pokud to nen√≠ nutn√©.\n"
        "- Neuv√°dƒõj vzorce ani k√≥d.\n"
        "- Form√°tuj odpovƒõƒè do odstavc≈Ø pro lep≈°√≠ ƒçitelnost.\n"
        "- Pokud nƒõkter√° metrika chyb√≠ (nap≈ô. R-squared u klasifikace), nekomentuj jej√≠ absenci, soust≈ôeƒè se na dostupn√© informace.\n"
        "- Pokud jsou koeficienty pro multinomi√°ln√≠ regresi, vysvƒõtli, ≈æe reprezentuj√≠ vliv na jednu z kategori√≠ oproti referenƒçn√≠."
    )

    # --- Sestaven√≠ User Promptu z dat od frontendu ---
    user_prompt_parts = [
        f"Provedl jsem anal√Ωzu typu '{req.analysis_type}' s pou≈æit√≠m metody '{req.method}'.",
        f"Z√°visl√° promƒõnn√° (Y): {req.dependent_variable}",
        f"Nez√°visl√© promƒõnn√© (X): {', '.join(req.independent_variables)}",
        "\nKl√≠ƒçov√© v√Ωsledky modelu:"
    ]
    if req.results_summary.r2 is not None:
        user_prompt_parts.append(f"- R-squared (R¬≤): {req.results_summary.r2:.3f}")
    if req.results_summary.r2_adjusted is not None:
        user_prompt_parts.append(f"- Adjusted R¬≤: {req.results_summary.r2_adjusted:.3f}")
    if req.results_summary.rmse is not None:
        user_prompt_parts.append(f"- RMSE (Root Mean Squared Error): {req.results_summary.rmse:.3f}")
    if req.results_summary.accuracy is not None:
        user_prompt_parts.append(f"- P≈ôesnost (Accuracy): {req.results_summary.accuracy * 100:.1f}%")
    if req.results_summary.overfitting_detected is not None:
        user_prompt_parts.append(f"- Detekov√°n mo≈æn√Ω overfitting: {'Ano' if req.results_summary.overfitting_detected else 'Ne'}")

    user_prompt_parts.append("\nOdhadnut√© koeficienty:")
    if req.intercept is not None:
         user_prompt_parts.append(f"- Intercept (pr≈Øseƒç√≠k): {req.intercept:.3f}")
    for coef in req.coefficients:
         user_prompt_parts.append(f"- {coef.name}: {coef.value:.3f}" if coef.value is not None else f"- {coef.name}: N/A")

    # P≈ôid√°n√≠ pozn√°mky pro multinomi√°ln√≠ regresi, pokud je to relevantn√≠
    if req.method == "multinomial":
         user_prompt_parts.append("\n(Pozn√°mka: Koeficienty u multinomi√°ln√≠ regrese ukazuj√≠ vliv promƒõnn√© na pravdƒõpodobnost jedn√© z kategori√≠ v≈Øƒçi referenƒçn√≠ kategorii.)")

    user_prompt_parts.append("\nPros√≠m, interpretuj tyto v√Ωsledky.")
    full_user_prompt = "\n".join(user_prompt_parts)

    # --- Vol√°n√≠ LLM API (podobnƒõ jako v ai_suggest_analysis) ---
    try:
        # POZOR: Zabezpeƒçte sv≈Øj API kl√≠ƒç! Neukl√°dejte ho p≈ô√≠mo v k√≥du.
        # Pou≈æijte nap≈ô. environment variables.
        api_key ="OPENROUTER_API_KEY"
        if not api_key:
             raise HTTPException(status_code=500, detail="Chyb√≠ konfigurace API kl√≠ƒçe pro LLM.")

        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek/deepseek-chat-v3-0324:free", # Nebo jin√Ω model
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_user_prompt}
                ],
                "max_tokens": 500 # M≈Ø≈æete upravit podle pot≈ôeby
            },
            timeout=60
        )
        response.raise_for_status() # Vyvol√° chybu pro status k√≥dy 4xx/5xx

        llm_data = response.json()

        # Extrakce textu odpovƒõdi
        interpretation_text = llm_data.get("choices", [{}])[0].get("message", {}).get("content")

        if not interpretation_text:
             raise HTTPException(status_code=500, detail="AI nevr√°tila platnou interpretaci.")

        # Vrac√≠me v√Ωsledek ve form√°tu oƒçek√°van√©m frontendem
        return {"interpretation": interpretation_text}

    except requests.exceptions.RequestException as req_err:
        print(f"Chyba komunikace s LLM API: {req_err}")
        raise HTTPException(status_code=503, detail=f"Chyba p≈ôi komunikaci s AI slu≈æbou: {req_err}")
    except Exception as e:
        print(f"Neoƒçek√°van√° chyba p≈ôi interpretaci: {e}")
        # M≈Ø≈æete logovat 'full_user_prompt' pro debugging
        raise HTTPException(status_code=500, detail=f"Intern√≠ chyba serveru p≈ôi generov√°n√≠ interpretace: {str(e)}")



class PromptRequest(BaseModel):
    prompt: str

@app.post("/api/ai_suggest_relationship")
async def ai_suggest_relationship(req: PromptRequest):
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="No stored data available.")

    headers = stored_datasets["latest"]["headers"]
    columns_text = ", ".join(headers)
    full_prompt = (
        f"M√°m dataset s n√°sleduj√≠c√≠mi sloupci: {columns_text}.\n\n"
        f"M≈Øj dotaz je: {req.prompt}"
    )

    system_prompt = (
        "Jsi n√°pomocn√Ω asistent pro anal√Ωzu vztah≈Ø mezi promƒõnn√Ωmi v datov√© sadƒõ.\n"
        "U≈æivatel pop√≠≈°e, co chce z dat zjistit.\n\n"
        "Tvoje √∫loha je doporuƒçit jeden z n√°sleduj√≠c√≠ch typ≈Ø anal√Ωzy:\n\n"
        "1. Korelace\n"
        "2. Regrese\n"
        "3. Test z√°vislosti\n\n"
        "Pravidla:\n"
        "- Odpovƒõz pouze jedn√≠m z tƒõchto n√°zv≈Ø.\n"
        "- P≈ôidej kr√°tk√© vysvƒõtlen√≠ (1‚Äì3 vƒõty), proƒç je dan√Ω typ vhodn√Ω.\n"
        "- Pokud nen√≠ jasn√©, co u≈æivatel chce, po≈æ√°dej o up≈ôesnƒõn√≠.\n"
        "- Odpovƒõƒè pi≈° v ƒçe≈°tinƒõ.\n\n"
        "Zak√°z√°no:\n"
        "- Nezmi≈àuj konkr√©tn√≠ statistick√© testy.\n"
        "- Nepi≈° k√≥d nebo rovnice.\n"
        "- Neh√°dej v√≠ce mo≈ænost√≠ ‚Äì vyber pouze jednu nebo po≈æ√°dej o up≈ôesnƒõn√≠."
    )

    try:
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": "Bearer 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",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek/deepseek-chat-v3-0324:free",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_prompt}
                ],
                "max_tokens": 300
            },
            timeout=60
        )

        return response.json()

    except Exception as e:
        return {"error": str(e)}



class TransformRequest(BaseModel):
    column: str
    method: str  # 'log', 'sqrt', 'boxcox'

@app.post("/api/transform_column")
async def transform_column(req: TransformRequest):
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    headers = stored_datasets["latest"]["headers"]
    data = stored_datasets["latest"]["data"]
    df = pd.DataFrame(data, columns=headers)

    if req.column not in df.columns:
        raise HTTPException(status_code=404, detail="Sloupec nebyl nalezen.")

    try:
        # p≈ôevedeme na ƒç√≠sla
        series = pd.to_numeric(df[req.column], errors="coerce")

        if req.method == "log":
            if (series <= 0).any():
                raise ValueError("Logaritmick√° transformace vy≈æaduje pouze kladn√© hodnoty.")
            transformed = np.log(series)
        elif req.method == "sqrt":
            if (series < 0).any():
                raise ValueError("Odmocnina nen√≠ definovan√° pro z√°porn√© hodnoty.")
            transformed = np.sqrt(series)
        elif req.method == "boxcox":
            # odstran√≠me nuly a z√°pory
            if (series <= 0).any():
                raise ValueError("Box-Cox transformace vy≈æaduje pouze kladn√© hodnoty.")
            # odstranit NaN pro boxcox
            non_na = series.dropna()
            transformed_non_na, _ = boxcox(non_na)
            # znovu vlo≈æ√≠me NaN zpƒõt na p≈Øvodn√≠ m√≠sta
            transformed = pd.Series(data=np.nan, index=series.index)
            transformed[non_na.index] = transformed_non_na
        else:
            raise ValueError("Nezn√°m√° transformaƒçn√≠ metoda.")

        # nahrad√≠me p≈Øvodn√≠ sloupec transformovan√Ωmi daty
        df[req.column] = transformed

        # p≈ôep√≠≈°eme zpƒõt do ulo≈æen√Ωch dat
        stored_datasets["latest"]["headers"] = list(df.columns)
        stored_datasets["latest"]["data"] = df.values.tolist()

        return {"message": f"Transformace sloupce '{req.column}' metodou '{req.method}' probƒõhla √∫spƒõ≈°nƒõ."}

    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        raise HTTPException(status_code=500, detail="Chyba p≈ôi transformaci dat.")


@app.get("/api/check_normality")
async def check_normality(preferred_test: Optional[str] = Query(None)):
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    headers = stored_datasets["latest"]["headers"]
    data = stored_datasets["latest"]["data"]
    df = pd.DataFrame(data, columns=headers)

    results = []

    for col in df.columns:
        try:
            original_series = pd.to_numeric(df[col], errors="coerce")
            series = original_series.dropna()

            if len(series) < 3:
                continue  # Nedostatek dat

            total_count = len(original_series)
            missing_count = original_series.isna().sum()
            has_missing = missing_count > 0

            # Detekce extr√©mn√≠ch hodnot
            z_scores = zscore(series)
            outlier_ratio = np.mean(np.abs(z_scores) > 3)
            has_many_outliers = outlier_ratio > 0.05

            test_used = ""
            p = None
            reason = ""

            if preferred_test == "shapiro" or (preferred_test is None and len(series) < 50):
                stat, p = shapiro(series)
                test_used = "Shapiro-Wilk"
                reason = "Test vybr√°n, proto≈æe poƒçet hodnot je men≈°√≠ ne≈æ 50."
            elif preferred_test == "ks" or preferred_test is None:
                standardized = (series - series.mean()) / series.std()
                stat, p = kstest(standardized, "norm")
                test_used = "Kolmogorov‚ÄìSmirnov"
                reason = "Test vybr√°n, proto≈æe poƒçet hodnot je 50 nebo v√≠ce."

            note_parts = [reason]
            if has_many_outliers:
                note_parts.append("Pozor: hodnƒõ outlier≈Ø, m≈Ø≈æe ovlivnit v√Ωsledek.")
            if has_missing:
                note_parts.append(f"Sloupec obsahuje {missing_count} chybƒõj√≠c√≠ch hodnot z {total_count}.")

            results.append({
                "column": col,
                "test": test_used,
                "pValue": float(f"{p:.6g}"),
                "isNormal": bool(p > 0.05),
                "warning": " ".join(note_parts) if note_parts else "-",
                "hasMissing": bool(has_missing)  # <- Tady je oprava
            })

        except Exception as e:
            print(f"Chyba ve sloupci {col}: {e}")
            continue
    stored_datasets["latest"]["normality"] = {
        result["column"]: result["isNormal"] for result in results
    }

    return {"results": results}

@app.get("/api/get_column_types")
async def get_column_types():
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    analyze_result = stored_datasets["latest"].get("column_types")
    if not analyze_result:
        raise HTTPException(status_code=400, detail="Data nebyla je≈°tƒõ analyzov√°na.")

    response = []
    for col, info in analyze_result.items():
        response.append({
            "name": col,
            "type": info["type"]
        })

    return response

class GroupComparisonRequest(BaseModel):
    numerical: List[str]
    categorical: List[str]
    paired: bool


@app.post("/api/group_comparison")
async def group_comparison(req: GroupComparisonRequest):
    numerical = req.numerical
    categorical = req.categorical
    paired = req.paired

    if "latest" not in stored_datasets:
        raise HTTPException(status_code=404, detail="Data nejsou nahran√°.") # 404 pro nenalezeno

    # Zkontroluj, jestli u≈æ m√°me v√Ωpoƒçet normality. Pokud ne, zavolej ho.
    if "normality" not in stored_datasets["latest"]:
        print("Informace o normalitƒõ nenalezena, poƒç√≠t√°m...")
        try:
            # Spust√≠me funkci pro v√Ωpoƒçet normality
            normality_result_full = await check_normality() # Z√≠sk√°n√≠ kompletn√≠ odpovƒõdi
            # Ulo≈æ√≠me v√Ωsledek (slovn√≠k {sloupec: bool}) do cache
            stored_datasets["latest"]["normality"] = {
                r["column"]: r["isNormal"] for r in normality_result_full.get("results", [])
            }
            print("Informace o normalitƒõ vypoƒç√≠t√°na a ulo≈æena.")
        except Exception as e:
             print(f"Chyba p≈ôi v√Ωpoƒçtu normality: {e}")
             # Pokud v√Ωpoƒçet sel≈æe, pokraƒçujeme s pr√°zdn√Ωm dict, co≈æ povede k neparametrick√Ωm test≈Øm
             stored_datasets["latest"]["normality"] = {}


    if not numerical or not categorical:
        raise HTTPException(status_code=400, detail="Mus√≠te vybrat numerick√© i kategori√°ln√≠ promƒõnn√©.")

    df = pd.DataFrame(stored_datasets["latest"]["data"], columns=stored_datasets["latest"]["headers"])
    # Z√≠sk√°n√≠ normality info bezpeƒçnƒõ, s default {} pokud neexistuje
    normality_info = stored_datasets["latest"].get("normality", {})

    results_by_group = []

    # --- Logika pro p√°rov√Ω test ---
    if paired:
        if len(numerical) != 2 or len(categorical) != 1:
            raise HTTPException(status_code=400, detail="P√°rov√° anal√Ωza vy≈æaduje v√Ωbƒõr p≈ôesnƒõ 2 ƒç√≠seln√Ωch a 1 kategori√°ln√≠ promƒõnn√©.")

        col1, col2 = numerical
        group_col = categorical[0] # P≈ôedpokl√°d√°me, ≈æe pro p√°rov√Ω test m√° smysl jen 1 skupina pro definici p√°r≈Ø (ID subjektu apod.) - zv√°≈æit, zda je to v√°≈° p≈ô√≠pad u≈æit√≠

        # Pokus o konverzi sloupc≈Ø na numerick√©, chyby budou NaN
        try:
            df[col1] = pd.to_numeric(df[col1], errors='coerce')
            df[col2] = pd.to_numeric(df[col2], errors='coerce')
        except KeyError as e:
             raise HTTPException(status_code=404, detail=f"Sloupec '{e}' nebyl nalezen v datech.")
        except Exception as e:
             # Obecn√° chyba p≈ôi konverzi
             raise HTTPException(status_code=400, detail=f"Chyba p≈ôi konverzi ƒç√≠seln√Ωch sloupc≈Ø: {e}")

        group_results = []

        # Iterujeme p≈ôes skupiny definovan√© kategori√°ln√≠ promƒõnnou
        # POZN√ÅMKA: U p√°rov√©ho testu se typicky testuje rozd√≠l nap≈ô√≠ƒç podm√≠nkami *pro stejn√© subjekty*.
        # Group_col by zde mƒõl identifikovat sp√≠≈°e subjekty nebo p√°ry, pokud testujete efekt *uvnit≈ô* subjekt≈Ø.
        # Pokud group_col definuje nez√°visl√© skupiny a vy chcete p√°rov√Ω test *v r√°mci ka≈æd√© skupiny*, logika je zde spr√°vn√°.
        # Zva≈æte, zda v√°≈° sc√©n√°≈ô odpov√≠d√° tomuto p≈ôedpokladu.
        if group_col not in df.columns:
             raise HTTPException(status_code=404, detail=f"Kategori√°ln√≠ sloupec '{group_col}' nebyl nalezen v datech.")

        for group_value, group_df in df.groupby(group_col):
            # Vytvo≈ôen√≠ p√°r≈Ø a odstranƒõn√≠ ≈ô√°dk≈Ø s jak√Ωmkoli NA v p√°ru
            paired_data = group_df[[col1, col2]].dropna()

            if len(paired_data) < 3: # Pot≈ôebujeme alespo≈à 3 p√°ry pro smyslupln√Ω test
                print(f"P≈ôeskakuji skupinu '{group_value}' pro p√°rov√Ω test - m√©nƒõ ne≈æ 3 platn√© p√°ry ({len(paired_data)}).")
                continue

            x_vals = paired_data[col1]
            y_vals = paired_data[col2]

            # ***** OPRAVA ZDE: Testujeme normalitu ROZD√çL≈Æ *****
            differences = x_vals - y_vals
            is_diff_normal = False # V√Ωchoz√≠ stav: nenorm√°ln√≠
            note = f"P√°rov√Ω test, skupina: {group_value}"
            test = "N/A"
            p = float('nan')
            stat = float('nan')

            try:
                # Pot≈ôebujeme alespo≈à 3 rozd√≠ly pro Shapiro test
                if len(differences) >= 3:
                    # O≈°et≈ôen√≠ p≈ô√≠padu, kdy jsou v≈°echny rozd√≠ly stejn√© (Shapiro sel≈æe)
                    if len(differences.unique()) == 1:
                        is_diff_normal = False # Nelze testovat normalitu, p≈ôedpokl√°d√°me neparametrick√Ω
                        note += ", v≈°echny rozd√≠ly stejn√©"
                    else:
                        stat_norm, p_norm = shapiro(differences)
                        is_diff_normal = p_norm > 0.05
                        print(f"  Normality test rozd√≠l≈Ø (skupina {group_value}): p = {p_norm:.4f}, isNormal = {is_diff_normal}")
                else:
                    # Pokud m√°me m√©nƒõ ne≈æ 3 rozd√≠ly (nemƒõlo by nastat kv≈Øli kontrole v√Ω≈°e, ale pro jistotu)
                     is_diff_normal = False
                     note += ", m√°lo dat pro test normality rozd√≠l≈Ø"


                # V√Ωbƒõr testu na z√°kladƒõ normality rozd√≠l≈Ø
                if is_diff_normal:
                    # Rozd√≠ly jsou norm√°lnƒõ rozlo≈æen√© -> P√°rov√Ω t-test
                    stat, p = ttest_rel(x_vals, y_vals)
                    test = "P√°rov√Ω t-test"
                    note += ", norm√°ln√≠ rozlo≈æen√≠ rozd√≠l≈Ø"
                else:
                    # Rozd√≠ly nejsou norm√°lnƒõ rozlo≈æen√© (nebo test selhal) -> Wilcoxon≈Øv test
                    # Wilcoxon vy≈æaduje N > ~5-10 pro spolehliv√© p-hodnoty, pro mal√© N m≈Ø≈æe d√°t varov√°n√≠
                    if len(differences) > 0: # Zajist√≠, ≈æe nevol√°me s pr√°zdn√Ωmi daty
                         # O≈°et≈ôen√≠ pro p≈ô√≠pad, kdy jsou v≈°echny rozd√≠ly nulov√© (Wilcoxon m≈Ø≈æe selhat)
                         if (differences == 0).all():
                             test = "Wilcoxon≈Øv test"
                             stat, p = float('nan'), 1.0 # Nen√≠ ≈æ√°dn√Ω rozd√≠l
                             note += ", v≈°echny rozd√≠ly nulov√©"
                         else:
                             # Pou≈æit√≠ korekce a metody pro zpracov√°n√≠ nul a vazeb
                             stat, p = wilcoxon(x_vals, y_vals, zero_method='zsplit', correction=True, mode='approx')
                             test = "Wilcoxon≈Øv test"
                             note += ", nenorm√°ln√≠ rozlo≈æen√≠ rozd√≠l≈Ø (nebo nelze testovat)"
                    else:
                         # Nemƒõlo by nastat, ale pro jistotu
                         test = "Wilcoxon≈Øv test (nebƒõ≈æel)"
                         note += ", ≈æ√°dn√° data pro Wilcoxon test"

            except ValueError as ve:
                 print(f"Chyba p≈ôi statistick√©m testu pro skupinu {group_value}: {ve}")
                 test = f"Chyba testu ({'t-test' if is_diff_normal else 'Wilcoxon'})"
                 note += f", chyba: {ve}"
                 stat, p = float('nan'), float('nan')
            except Exception as e:
                 print(f"Neoƒçek√°van√° chyba p≈ôi statistick√©m testu pro skupinu {group_value}: {e}")
                 test = "Neoƒçek√°van√° chyba testu"
                 note += f", chyba: {e}"
                 stat, p = float('nan'), float('nan')


            group_results.append({
                "numericColumn": f"{col1} vs {col2}", # Jasnƒõj≈°√≠ oznaƒçen√≠
                "test": test,
                "statistic": float(f"{stat:.6g}") if not np.isnan(stat) else None, # P≈ôid√°n√≠ statistiky
                "pValue": float(f"{p:.6g}") if not np.isnan(p) else None, # Zajist√≠ None m√≠sto NaN pro JSON
                "isSignificant": bool(not np.isnan(p) and p < 0.05),
                "note": note
            })

        # Struktura v√Ωsledk≈Ø pro p√°rov√Ω test (agregov√°no p≈ôes skupiny)
        results_by_group.append({
            "groupColumn": group_col, # N√°zev sloupce, kter√Ω definoval "skupiny" pro test
            "type": "paired",
            "results": group_results
        })

    # --- Logika pro nez√°visl√© skupiny (z≈Øst√°v√° stejn√°) ---
    else:
        for group_col in categorical:
            if group_col not in df.columns:
                print(f"Varov√°n√≠: Kategori√°ln√≠ sloupec '{group_col}' nebyl nalezen, p≈ôeskakuji.")
                continue

            group_results = []

            for num_col in numerical:
                if num_col not in df.columns:
                     print(f"Varov√°n√≠: Numerick√Ω sloupec '{num_col}' nebyl nalezen, p≈ôeskakuji pro skupinu '{group_col}'.")
                     continue

                test = "N/A"
                p = float('nan')
                stat = float('nan')
                reason = ""
                is_significant = False

                try:
                    # Pokus o konverzi numerick√©ho sloupce ZDE, abychom nezmƒõnili p≈Øvodn√≠ df pro dal≈°√≠ iterace
                    numeric_data_series = pd.to_numeric(df[num_col], errors='coerce')
                    # Vytvo≈ôen√≠ subsetu s aktu√°ln√≠ kategori√°ln√≠ a numerickou promƒõnnou
                    subset = pd.concat([df[group_col], numeric_data_series], axis=1).dropna()

                    if subset.empty or subset[group_col].nunique() < 2:
                        print(f"P≈ôeskakuji {num_col} vs {group_col} - nedostatek dat nebo m√©nƒõ ne≈æ 2 skupiny po odstranƒõn√≠ NA.")
                        continue

                    unique_groups = subset[group_col].unique()
                    # Vytvo≈ôen√≠ seznamu Series pro ka≈ædou skupinu
                    values = [subset[subset[group_col] == g][num_col] for g in unique_groups]

                    # Z√≠sk√°n√≠ informace o normalitƒõ z cache (s default False)
                    is_col_normal = normality_info.get(num_col, False)
                    equal_var = True # V√Ωchoz√≠ p≈ôedpoklad

                    if len(unique_groups) == 2:
                        # Levene test pro 2 skupiny - jen pokud m√°me dostatek dat v obou skupin√°ch
                        if all(len(v) >= 3 for v in values): # Levene pot≈ôebuje alespo≈à p√°r bod≈Ø
                           try:
                               stat_levene, p_levene = levene(*values)
                               equal_var = p_levene > 0.05
                               reason += f"Levene p={p_levene:.3f}. "
                           except ValueError as ve:
                               print(f"Levene test selhal pro {num_col} vs {group_col}: {ve}. P≈ôedpokl√°d√°m nerovnost rozptyl≈Ø.")
                               equal_var = False # Pokud test sel≈æe, bezpeƒçnƒõj≈°√≠ p≈ôedpokl√°dat nerovnost
                               reason += "Levene test selhal. "
                        else:
                           equal_var = False # M√°lo dat pro Levene, p≈ôedpokl√°d√°me nerovnost
                           reason += "M√°lo dat pro Levene test. "


                        # V√Ωbƒõr testu pro 2 skupiny
                        if is_col_normal and equal_var:
                            stat, p = ttest_ind(*values, equal_var=True)
                            test = "t-test (nez√°visl√Ω)"
                            reason += "2 skupiny, norm√°ln√≠ data*, homogenn√≠ rozptyly"
                        elif is_col_normal: # equal_var je False
                            stat, p = ttest_ind(*values, equal_var=False)
                            test = "Welch≈Øv t-test"
                            reason += "2 skupiny, norm√°ln√≠ data*, rozd√≠ln√© rozptyly"
                        else: # Nenorm√°ln√≠ data
                            stat, p = mannwhitneyu(*values, alternative='two-sided') # Explicitnƒõ two-sided
                            test = "Mann‚ÄìWhitney U"
                            reason += "2 skupiny, nenorm√°ln√≠ data*"
                    elif len(unique_groups) > 2:
                        # Levene test pro >2 skupiny
                        if all(len(v) >= 3 for v in values):
                            try:
                                stat_levene, p_levene = levene(*values)
                                equal_var = p_levene > 0.05
                                reason += f"Levene p={p_levene:.3f}. "
                            except ValueError as ve:
                                print(f"Levene test selhal pro {num_col} vs {group_col}: {ve}. P≈ôedpokl√°d√°m nerovnost rozptyl≈Ø.")
                                equal_var = False
                                reason += "Levene test selhal. "
                        else:
                            equal_var = False
                            reason += "M√°lo dat pro Levene test. "

                        # V√Ωbƒõr testu pro >2 skupiny
                        if is_col_normal and equal_var:
                            stat, p = f_oneway(*values)
                            test = "ANOVA"
                            reason += "v√≠ce skupin, norm√°ln√≠ data*, homogenn√≠ rozptyly"
                        else: # Nenorm√°ln√≠ data NEBO rozd√≠ln√© rozptyly
                            stat, p = kruskal(*values)
                            test = "Kruskal‚ÄìWallis"
                            reason += "v√≠ce skupin, nenorm√°ln√≠ data* nebo rozd√≠ln√© rozptyly"
                    else: # M√©nƒõ ne≈æ 2 skupiny (nemƒõlo by nastat kv≈Øli kontrole v√Ω≈°e)
                        continue

                    is_significant = bool(not np.isnan(p) and p < 0.05)

                except ValueError as ve:
                     print(f"Chyba p≈ôi statistick√©m testu pro {num_col} vs {group_col}: {ve}")
                     test = f"Chyba testu"
                     reason += f", chyba: {ve}"
                     stat, p, is_significant = float('nan'), float('nan'), False
                except Exception as e:
                     print(f"Neoƒçek√°van√° chyba p≈ôi statistick√©m testu pro {num_col} vs {group_col}: {e}")
                     test = "Neoƒçek√°van√° chyba testu"
                     reason += f", chyba: {e}"
                     stat, p, is_significant = float('nan'), float('nan'), False


                group_results.append({
                    "numericColumn": num_col,
                    "test": test,
                    "statistic": float(f"{stat:.6g}") if not np.isnan(stat) else None,
                    "pValue": float(f"{p:.6g}") if not np.isnan(p) else None,
                    "isSignificant": is_significant,
                    "note": reason.strip() + " (*dle p≈ôedpoƒç√≠tan√© normality)" # Dodatek k p≈Øvodu normality
                })

            # P≈ôid√°n√≠ v√Ωsledk≈Ø pro aktu√°ln√≠ kategori√°ln√≠ promƒõnnou
            if group_results: # P≈ôid√°me jen pokud m√°me nƒõjak√© v√Ωsledky
                results_by_group.append({
                    "groupColumn": group_col,
                    "type": "independent",
                    "results": group_results
                })

    if not results_by_group:
         # Pokud se neprovedl ≈æ√°dn√Ω test (nap≈ô. kv≈Øli nedostatku dat)
         raise HTTPException(status_code=400, detail="Nepoda≈ôilo se prov√©st ≈æ√°dn√© porovn√°n√≠ skupin. Zkontrolujte data a v√Ωbƒõr promƒõnn√Ωch.")


    return {"results": results_by_group}

@app.post("/api/update_column_type")
async def update_column_type(req: Request):
    body = await req.json()
    column = body.get("column")
    new_type = body.get("newType")

    if not column or new_type not in ["Kategorie", "ƒå√≠seln√Ω"]:
        raise HTTPException(status_code=400, detail="Neplatn√Ω vstup")

    if "latest" not in stored_datasets or "column_types" not in stored_datasets["latest"]:
        raise HTTPException(status_code=400, detail="Data nejsou p≈ôipravena")

    if column not in stored_datasets["latest"]["column_types"]:
        raise HTTPException(status_code=404, detail="Sloupec nenalezen")

    stored_datasets["latest"]["column_types"][column]["type"] = new_type
    return {"status": "updated"}

@app.get("/api/get_outliers")
async def get_outliers():
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    headers = stored_datasets["latest"]["headers"]
    data = stored_datasets["latest"]["data"]

    df = pd.DataFrame(data, columns=headers)
    df_numeric = df.apply(pd.to_numeric, errors='coerce')

    outliers_summary = []

    for col in df_numeric.columns:
        series = df_numeric[col].dropna()
        if series.empty:
            continue

        mean = series.mean()
        std = series.std()
        if std == 0:
            continue

        z_scores = (series - mean) / std
        outliers = series[abs(z_scores) > 3]

        outliers_summary.append({
            "column": col,
            "count": int(len(outliers)),
            "percent": float(len(outliers)) / len(series) * 100,
            "mean": float(mean),
            "std": float(std),
            "values": series.tolist(),
            "outliers": outliers.tolist()
        })

    return outliers_summary

# @app.post("/api/analyze_with_llm")
# async def analyze_with_llm(request: DataRequest):
#     df = pd.DataFrame(request.data, columns=request.headers)
#
#     # üöÄ Ode≈°leme cel√Ω dataset (ale max. 500 ≈ô√°dk≈Ø, pokud je velk√Ω)
#     if len(df) > 500:
#         df = df.sample(500, random_state=42)
#
#     # ‚úÖ P≈ôevod na CSV form√°t (lep≈°√≠ pro LLM ne≈æ JSON)
#     csv_data = df.to_csv(index=False)
#
#     def stream_llm_response():
#         try:
#             print("üü° Odes√≠l√°m dotaz na LLM...")
#
#             response = requests.post(
#                 "http://127.0.0.1:1234/v1/chat/completions",
#                 json={
#                     #"model": "deepseek-r1-distill-llama-8b",
#                     "model": "hermes-3-llama-3.1-8b",
#                     "messages": [{"role": "user", "content": f"{csv_data}"}],
#                     "max_tokens": 200,
#                     "stream": True
#                 },
#                 timeout=60,
#                 stream=True
#             )
#
#             print("üü¢ LLM odpovƒõƒè zaƒç√≠n√° streamovat...")
#             full_response = ""
#             last_char = ""
#
#             for line in response.iter_lines():
#                 if line:
#                     decoded_line = line.decode("utf-8").strip()
#                     print(f"üîπ P≈ôijat√Ω ≈ô√°dek: {decoded_line}")
#
#                     if decoded_line.startswith("data:"):
#                         decoded_line = decoded_line.replace("data: ", "")
#
#                     try:
#                         json_data = json.loads(decoded_line)
#
#                         if "choices" in json_data and json_data["choices"]:
#                             content_chunk = json_data["choices"][0]["delta"].get("content", "")
#
#                             if content_chunk:
#                                 # ‚úÖ Pokud posledn√≠ znak nen√≠ mezera, ale p≈ôich√°z√≠ dal≈°√≠ text, p≈ôid√°me mezeru
#                                 if last_char not in ["", " ", "\n"] and content_chunk[0] not in [".", ",", "!", "?",
#                                                                                                  ";", ":"]:
#                                     content_chunk = " " + content_chunk
#
#                                 full_response += content_chunk
#                                 last_char = content_chunk[-1] if content_chunk else last_char
#
#                                 print(f"üìù Obsah: {content_chunk}")
#                                 yield content_chunk
#
#                     except (KeyError, json.JSONDecodeError):
#                         print("‚ö†Ô∏è Chyba p≈ôi parsov√°n√≠ JSON, pokraƒçuji...")
#                         continue
#
#             print("‚úÖ Streamov√°n√≠ dokonƒçeno. Cel√° odpovƒõƒè:")
#             print(full_response)
#
#         except requests.exceptions.RequestException as e:
#             print(f"‚ùå Chyba p≈ôi komunikaci s LLM: {str(e)}")
#             yield f"‚ùå Chyba p≈ôi komunikaci s LLM: {str(e)}\n"
#
#     return StreamingResponse(stream_llm_response(), media_type="text/event-stream")
#
@app.post("/api/analyze_with_llm")
async def analyze_with_llm(request: DataRequest):
    df = pd.DataFrame(request.data, columns=request.headers)
    if len(df) > 500:
        df = df.sample(500, random_state=42)

    csv_data = df.to_csv(index=False)
    def stream_llm_response():
        try:
            print("Odes√≠l√°m dotaz na LLM...")
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": "Bearer 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",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "deepseek/deepseek-chat-v3-0324:free",
                             "messages": [
                        {
                            "role": "system",
                            "content": (
                                "Classify each column of the input table as either numerical or categorical.\n\n"
                                "Respond with a single line of letters:\n\n"
                                "Use \"c\" for numerical columns.\n\n"
                                "Use \"k\" for categorical columns.\n\n"
                                "Separate each letter with a semicolon (;).\n\n"
                                "There HAS TO BE a semicolon after each letter\n\n"
                                "The total number of letters must match the number of columns in the dataset.\n"
                                "Do not include column names, explanations, or any other text.\n\n"
                                "Forbidden: column names, descriptions, reasoning\n"
                                "Allowed: c;k;"
                            )
                        },
                        {
                            "role": "user",
                            "content": csv_data
                        }
                    ],
                    "max_tokens": 200,
                    "stream": True
                },
                timeout=60,
                stream=True
            )

            print("üü¢ LLM odpovƒõƒè zaƒç√≠n√° streamovat...")
            full_response = ""
            last_char = ""

            for line in response.iter_lines():
                if line:
                    decoded_line = line.decode("utf-8").strip()
                    print(f"üîπ P≈ôijat√Ω ≈ô√°dek: {decoded_line}")

                    if decoded_line.startswith("data: "):
                        decoded_line = decoded_line.replace("data: ", "")

                    try:
                        json_data = json.loads(decoded_line)

                        if "choices" in json_data and json_data["choices"]:
                            content_chunk = json_data["choices"][0]["delta"].get("content", "")

                            if content_chunk:
                                if last_char not in ["", " ", "\n"] and content_chunk[0] not in [".", ",", "!", "?", ";", ":"]:
                                    content_chunk = " " + content_chunk

                                full_response += content_chunk
                                last_char = content_chunk[-1] if content_chunk else last_char

                                print(f"üìù Obsah: {content_chunk}")
                                yield content_chunk

                    except (KeyError, json.JSONDecodeError):
                        print("‚ö†Ô∏è Chyba p≈ôi parsov√°n√≠ JSON, pokraƒçuji...")
                        continue

            print("‚úÖ Streamov√°n√≠ dokonƒçeno. Cel√° odpovƒõƒè:")
            print(full_response)

        except requests.exceptions.RequestException as e:
            print(f"‚ùå Chyba p≈ôi komunikaci s LLM: {str(e)}")
            yield f"‚ùå Chyba p≈ôi komunikaci s LLM: {str(e)}\n"

    return StreamingResponse(stream_llm_response(), media_type="text/event-stream")

class UpdateTypeRequest(BaseModel):
    column: str
    newType: str # "Kat
    

@app.post("/api/validate_and_update_column_type") # Nov√Ω n√°zev endpointu je lep≈°√≠
async def validate_and_update_column_type(req: UpdateTypeRequest):
    column = req.column
    new_type = req.newType
    logger.info(f"Received request to validate and update type for column '{column}' to '{new_type}'") # Log

    # --- Z√°kladn√≠ validace vstupu ---
    if new_type not in ["Kategorie", "ƒå√≠seln√Ω"]:
        logger.error(f"Invalid newType received: {new_type}")
        raise HTTPException(status_code=400, detail="Neplatn√Ω c√≠lov√Ω typ sloupce.")

    if "latest" not in stored_datasets:
        logger.error("Attempted to update type but no data is stored.")
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    headers = stored_datasets["latest"].get("headers")
    data = stored_datasets["latest"].get("data")
    column_types_info = stored_datasets["latest"].get("column_types", {})

    if not headers or data is None: # Zkontrolujte i data
        logger.error("Stored data is incomplete (missing headers or data).")
        raise HTTPException(status_code=400, detail="Ulo≈æen√° data jsou nekompletn√≠.")

    if column not in headers:
        logger.error(f"Column '{column}' not found in headers.")
        raise HTTPException(status_code=404, detail=f"Sloupec '{column}' nebyl v datech nalezen.")

    if column not in column_types_info:
         # Pokud typy je≈°tƒõ nebyly analyzov√°ny, nem≈Ø≈æeme mƒõnit
         logger.error(f"Column types not analyzed yet, cannot update type for '{column}'.")
         raise HTTPException(status_code=400, detail="Typy sloupc≈Ø je≈°tƒõ nebyly analyzov√°ny.")


    # --- Validace konverze dat (pokud se mƒõn√≠ na ƒå√≠seln√Ω) ---
    if new_type == "ƒå√≠seln√Ω":
        try:
            # Z√≠sk√°n√≠ indexu sloupce
            col_index = headers.index(column)
            # Extrakce hodnot sloupce (efektivnƒõj≈°√≠ ne≈æ tvo≈ôit cel√Ω DataFrame)
            column_values = [row[col_index] for row in data if row is not None and len(row) > col_index]

            # Vytvo≈ôen√≠ Pandas Series pro snadnƒõj≈°√≠ validaci
            series = pd.Series(column_values)
            # Nahrazen√≠ pr√°zdn√Ωch string≈Ø a None konzistentnƒõ NaN
            series = series.replace('', np.nan).astype(object) #astype(object) for mixed types initially

            # Pokus o konverzi na ƒç√≠slo, chyby budou NaN
            numeric_series = pd.to_numeric(series, errors='coerce')

            # Zji≈°tƒõn√≠, zda P≈ÆVODN√ç ne-NaN hodnoty selhaly p≈ôi konverzi
            original_non_na_mask = series.notna()
            conversion_failed_mask = numeric_series.isna() & original_non_na_mask

            if conversion_failed_mask.any():
                # Najdi p√°r p≈ô√≠klad≈Ø hodnot, kter√© selhaly
                failed_examples = series[conversion_failed_mask].unique()
                examples_str = ", ".join(map(str, failed_examples[:5])) # Uka≈æ max 5 p≈ô√≠klad≈Ø
                error_msg = f"Sloupec '{column}' nelze p≈ôev√©st na ƒç√≠seln√Ω typ, proto≈æe obsahuje neƒç√≠seln√© hodnoty (nap≈ô.: {examples_str})."
                logger.warning(f"Validation failed for column '{column}' to 'ƒå√≠seln√Ω': {error_msg}")
                raise HTTPException(status_code=400, detail=error_msg) # Vr√°t√≠me chybu 400

            logger.info(f"Validation successful for column '{column}' to 'ƒå√≠seln√Ω'.")

        except HTTPException as http_exc:
             raise http_exc # Propagujeme HTTP chybu d√°l
        except Exception as e:
            logger.error(f"Unexpected error during validation for column '{column}': {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Nastala chyba p≈ôi validaci dat sloupce '{column}': {str(e)}")

    # --- Aktualizace typu v `stored_datasets`, pokud validace pro≈°la (nebo se mƒõnilo na Kategorie) ---
    stored_datasets["latest"]["column_types"][column]["type"] = new_type
    logger.info(f"Successfully updated type for column '{column}' to '{new_type}'.")

    # Vr√°t√≠me √∫spƒõ≈°nou odpovƒõƒè
    return {"status": "updated", "column": column, "newType": new_type}
@app.post("/api/handle_outliers")
async def handle_outliers(request: Request):
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=404, detail="No data uploaded.")

    body = await request.json()
    column = body.get("column")
    method = body.get("method")
    custom_value = body.get("custom_value")

    headers = stored_datasets["latest"]["headers"]
    raw_data = stored_datasets["latest"]["data"]
    df = pd.DataFrame(raw_data, columns=headers)

    # Pomocn√° funkce pro zpracov√°n√≠ jednoho sloupce
    def handle_column(col):
        values = pd.to_numeric(df[col], errors='coerce')
        mean = values.mean()
        std = values.std()
        z_scores = (values - mean) / std
        outlier_mask = z_scores.abs() > 2

        if method == "remove":
            return ~outlier_mask  # Vrac√≠ masku pro ponechan√© ≈ô√°dky

        elif method == "replace_mean":
            df.loc[outlier_mask, col] = mean
        elif method == "replace_median":
            df.loc[outlier_mask, col] = values.median()
        elif method == "replace_custom":
            try:
                replacement = float(custom_value)
                df.loc[outlier_mask, col] = replacement
            except:
                raise HTTPException(status_code=400, detail="Invalid custom value")
        elif method == "clip":
            lower_limit = mean - 2 * std
            upper_limit = mean + 2 * std
            df.loc[values < lower_limit, col] = lower_limit
            df.loc[values > upper_limit, col] = upper_limit

        return None

    if column == "ALL":
        if method == "remove":
            # vytvo≈ô kombinovanou masku pro v≈°echny sloupce
            keep_mask = pd.Series([True] * len(df))
            for col in df.columns:
                col_mask = handle_column(col)
                if col_mask is not None:
                    keep_mask &= col_mask
            df = df[keep_mask]
        else:
            for col in df.columns:
                handle_column(col)
    else:
        if column not in df.columns:
            raise HTTPException(status_code=400, detail="Column not found.")
        if method == "remove":
            keep_mask = handle_column(column)
            if keep_mask is not None:
                df = df[keep_mask]
        else:
            handle_column(column)

    stored_datasets["latest"] = {
        "headers": df.columns.tolist(),
        "data": df.astype(str).where(pd.notna(df), "").values.tolist()
    }

    return {"status": "outliers handled"}

class FactorAnalysisRequest(BaseModel):
    columns: List[str] = Field(..., min_length=3)
    n_factors: Optional[int] = Field(None, gt=0)
    rotation: Optional[str] = "varimax"
    standardize: bool = True

class DataAdequacy(BaseModel):
    kmo_model: Optional[float] = None
    bartlett_chi_square: Optional[float] = None
    bartlett_p_value: Optional[float] = None

class FactorVarianceItem(BaseModel):
    factor: str
    ssl: float
    variance_pct: float
    cumulative_variance_pct: float

class FactorAnalysisResult(BaseModel):
    columns_used: List[str]
    n_factors_requested: Optional[int]
    n_factors_extracted: int
    eigenvalue_criterion_used: bool
    eigenvalues: Optional[List[Optional[float]]] = None # Allow None within the list too
    rotation_used: str
    standardized: bool
    dropped_rows: int
    data_adequacy: DataAdequacy
    factor_loadings: Dict[str, Dict[str, Optional[float]]] # Allow None for loadings
    factor_variance: List[FactorVarianceItem]
    total_variance_explained_pct: Optional[float] = None
    communalities: Dict[str, Optional[float]] # Allow None for communalities

# --- Helper Function for NaN/Inf (Add this if you don't have a similar one) ---
def safe_float(value: Any) -> Optional[float]:
    """Converts value to float, returning None if NaN, Inf, or conversion fails."""
    try:
        # Check if value is already None or an empty string which pandas might produce
        if value is None or value == '':
            return None
        f_val = float(value)
        if math.isnan(f_val) or math.isinf(f_val):
            return None
        return f_val
    except (ValueError, TypeError):
        return None


@app.post("/api/factor_analysis", response_model=FactorAnalysisResult)
async def run_factor_analysis(request: FactorAnalysisRequest):
    """
    Performs Factor Analysis on selected numeric columns of the stored dataset.
    """
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    # Get data from your global store
    headers = stored_datasets["latest"].get("headers")
    data = stored_datasets["latest"].get("data")
    column_types_info = stored_datasets["latest"].get("column_types", {}) # Get type info if available

    if not headers or not data:
        raise HTTPException(status_code=400, detail="Chyb√≠ hlaviƒçky nebo data v ulo≈æen√©m datasetu.")

    df_full = pd.DataFrame(data, columns=headers)
    if df_full.empty:
        raise HTTPException(status_code=404, detail="Dataset je pr√°zdn√Ω.")

    # --- Input Validation ---
    # Frontend already checks for min 3, but good practice to double-check
    if len(request.columns) < 3:
         raise HTTPException(status_code=400, detail="Pro faktorovou anal√Ωzu vyberte alespo≈à 3 promƒõnn√©.")

    missing_cols = [col for col in request.columns if col not in df_full.columns]
    if missing_cols:
        raise HTTPException(status_code=404, detail=f"Sloupce nenalezeny v datasetu: {', '.join(missing_cols)}")

    # Check if selected columns are considered numeric based on stored types (optional but good)
    non_numeric_selected = []
    if column_types_info:
        for col in request.columns:
            col_info = column_types_info.get(col)
            if col_info and col_info.get("type") != "ƒå√≠seln√Ω":
                non_numeric_selected.append(col)
        if non_numeric_selected:
             logger.warning(f"Pokou≈°√≠te se prov√©st FA na sloupc√≠ch, kter√© nebyly detekov√°ny jako ƒç√≠seln√©: {', '.join(non_numeric_selected)}. V√Ωsledky mohou b√Ωt nespr√°vn√©.")
             # You could raise an HTTPException here if you want to be strict
             # raise HTTPException(status_code=400, detail=f"Vybran√© sloupce nejsou ƒç√≠seln√©: {', '.join(non_numeric_selected)}")


    # Select data and ensure numeric types
    try:
        df_selected = df_full[request.columns].copy()
        # Attempt to convert columns to numeric, coercing errors will result in NaN
        for col in request.columns:
             # Replace empty strings with NaN before converting
             df_selected[col] = df_selected[col].replace('', np.nan)
             df_selected[col] = pd.to_numeric(df_selected[col], errors='coerce')

        # Check if any column is entirely NaN after coercion
        if df_selected.isnull().all().any():
             bad_cols = df_selected.columns[df_selected.isnull().all()].tolist()
             raise ValueError(f"Vybran√© sloupce obsahuj√≠ pouze neƒç√≠seln√© hodnoty nebo jsou pr√°zdn√©: {', '.join(bad_cols)}")

    except ValueError as e:
         raise HTTPException(status_code=400, detail=f"Chyba p≈ôi v√Ωbƒõru nebo konverzi sloupc≈Ø na numerick√Ω typ: {e}")
    except Exception as e:
        logger.error(f"Neoƒçek√°van√° chyba p≈ôi p≈ô√≠pravƒõ FA dat: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Chyba p≈ôi p≈ô√≠pravƒõ dat pro FA: {e}")

    # --- Data Preprocessing ---
    initial_rows = len(df_selected)
    df_clean = df_selected.dropna()
    dropped_rows = initial_rows - len(df_clean)

    if len(df_clean) < len(request.columns) + 1:
         raise HTTPException(status_code=400, detail=f"Nedostatek validn√≠ch dat po odstranƒõn√≠ {dropped_rows} ≈ô√°dk≈Ø s chybƒõj√≠c√≠mi hodnotami ({len(df_clean)} ≈ô√°dk≈Ø). Pot≈ôeba alespo≈à {len(request.columns) + 1}.")
    if df_clean.empty:
         raise HTTPException(status_code=400, detail="Po odstranƒõn√≠ chybƒõj√≠c√≠ch hodnot nez≈Østala ≈æ√°dn√° data.")

    # Standardize if requested
    data_to_analyze = df_clean # Keep as DataFrame for easier handling later
    if request.standardize:
        try:
            scaler = StandardScaler()
            # Fit and transform, result is numpy array
            scaled_values = scaler.fit_transform(df_clean.values)
            # Convert back to DataFrame with original columns and index
            data_to_analyze = pd.DataFrame(scaled_values, columns=df_clean.columns, index=df_clean.index)
            logger.info("Data pro FA byla standardizov√°na.")
        except Exception as e:
             logger.error(f"Chyba p≈ôi standardizaci dat pro FA: {e}", exc_info=True)
             raise HTTPException(status_code=500, detail=f"Chyba p≈ôi standardizaci dat: {e}")

    # --- Adequacy Tests ---
    kmo_all, kmo_model = None, None
    bartlett_chi_square, bartlett_p_value = None, None
    try:
        # Check for zero variance
        if (data_to_analyze.var() < 1e-10).any(): # Check for near-zero variance
             zero_var_cols = data_to_analyze.columns[data_to_analyze.var() < 1e-10].tolist()
             logger.warning(f"Sloupce s nulovou nebo t√©mƒõ≈ô nulovou varianc√≠ nalezeny: {zero_var_cols}. FA m≈Ø≈æe selhat nebo b√Ωt nespolehliv√°.")
             # Don't calculate tests if variance is zero, they will likely fail

        else:
            # KMO Test - requires DataFrame
            try:
                kmo_per_variable, kmo_model = calculate_kmo(data_to_analyze)
                kmo_model = safe_float(kmo_model)
            except np.linalg.LinAlgError:
                 logger.warning("KMO test selhal - pravdƒõpodobnƒõ singul√°rn√≠ matice.")
                 kmo_model = None
            except ValueError as e:
                 logger.warning(f"KMO test selhal: {e}")
                 kmo_model = None


            # Bartlett's Test - requires DataFrame
            try:
                 chi_square, p_value = calculate_bartlett_sphericity(data_to_analyze)
                 bartlett_chi_square = safe_float(chi_square)
                 bartlett_p_value = safe_float(p_value)
            except ValueError as e:
                 logger.warning(f"Bartlett≈Øv test selhal: {e}")
                 bartlett_chi_square = None
                 bartlett_p_value = None


    except Exception as e:
        logger.error(f"Neoƒçek√°van√° chyba bƒõhem adequacy test≈Ø: {e}", exc_info=True)
        # Continue analysis, but tests will be None

    adequacy_results = DataAdequacy(
        kmo_model=kmo_model,
        bartlett_chi_square=bartlett_chi_square,
        bartlett_p_value=bartlett_p_value
    )

    # --- Determine Number of Factors ---
    n_factors_extracted = 0
    eigenvalues_list: Optional[List[Optional[float]]] = None
    eigenvalue_criterion_used = False

    if request.n_factors:
        n_factors_extracted = request.n_factors
        if n_factors_extracted >= len(request.columns):
             raise HTTPException(
                 status_code=400,
                 detail=f"Poƒçet po≈æadovan√Ωch faktor≈Ø ({n_factors_extracted}) nem≈Ø≈æe b√Ωt roven nebo vƒõt≈°√≠ ne≈æ poƒçet promƒõnn√Ωch ({len(request.columns)})."
             )
        logger.info(f"Pou≈æije se {n_factors_extracted} faktor≈Ø dle po≈æadavku u≈æivatele.")
    else:
        eigenvalue_criterion_used = True
        logger.info("Urƒçen√≠ poƒçtu faktor≈Ø pomoc√≠ Kaiserova krit√©ria (eigenvalue > 1).")
        try:
            # Calculate eigenvalues from the CORRELATION matrix of the processed data
            correlation_matrix = data_to_analyze.corr().values
            eigenvalues, _ = np.linalg.eigh(correlation_matrix) # Use eigh for symmetric matrices
            eigenvalues_sorted = sorted(eigenvalues.tolist(), reverse=True) # Convert to list
            eigenvalues_list = [safe_float(ev) for ev in eigenvalues_sorted]

            # Count eigenvalues > 1 (handle None from safe_float)
            n_factors_extracted = sum(ev > 1 for ev in eigenvalues_list if ev is not None)

            if n_factors_extracted == 0:
                logger.warning("≈Ω√°dn√© eigenvalue > 1. Extrahuji 1 faktor.")
                n_factors_extracted = 1
            elif n_factors_extracted >= len(request.columns):
                logger.warning(f"Kaiserovo krit√©rium navrhlo {n_factors_extracted} faktor≈Ø (>= poƒçtu promƒõnn√Ωch). Redukuji na {max(1, len(request.columns) - 1)}.")
                n_factors_extracted = max(1, len(request.columns) - 1)
            else:
                 logger.info(f"Kaiserovo krit√©rium navrhlo {n_factors_extracted} faktor≈Ø.")

        except np.linalg.LinAlgError:
             logger.error("Chyba p≈ôi v√Ωpoƒçtu eigenvalues (singul√°rn√≠ matice?)", exc_info=True)
             raise HTTPException(status_code=500, detail="Chyba p≈ôi v√Ωpoƒçtu eigenvalues (pravdƒõpodobnƒõ singul√°rn√≠ korelaƒçn√≠ matice). Zkuste jin√© promƒõnn√©.")
        except Exception as e:
             logger.error(f"Neoƒçek√°van√° chyba p≈ôi urƒçov√°n√≠ poƒçtu faktor≈Ø: {e}", exc_info=True)
             raise HTTPException(status_code=500, detail=f"Neoƒçek√°van√° chyba p≈ôi urƒçov√°n√≠ poƒçtu faktor≈Ø: {e}")

    # --- Fit Factor Analysis Model ---
    # Map 'none' rotation from frontend to None for the library
    rotation_param = None if request.rotation == 'none' else request.rotation

    fa = FactorAnalyzer(
        n_factors=n_factors_extracted,
        rotation=rotation_param,
        method='minres', # Principal Axis Factoring (minimum residual) is common
        use_smc=True
    )

    try:
        logger.info(f"Spou≈°t√≠m FactorAnalyzer s {n_factors_extracted} faktory a rotac√≠ '{rotation_param}'...")
        # Fit requires numpy array or DataFrame
        fa.fit(data_to_analyze)
        logger.info("FactorAnalyzer fit dokonƒçen.")
    except ValueError as e:
         logger.error(f"Chyba p≈ôi fitov√°n√≠ modelu Factor Analysis: {e}", exc_info=True)
         raise HTTPException(status_code=400, detail=f"Chyba p≈ôi fitov√°n√≠ modelu Factor Analysis: {str(e)}. Zkuste m√©nƒõ faktor≈Ø, jin√© promƒõnn√©, jinou rotaci nebo zkontrolujte data (nap≈ô. nulov√° variance).")
    except np.linalg.LinAlgError as e:
         logger.error(f"Line√°rn√≠ algebra chyba p≈ôi fitov√°n√≠ FA: {e}", exc_info=True)
         raise HTTPException(status_code=400, detail=f"Chyba line√°rn√≠ algebry p≈ôi fitov√°n√≠ FA: {str(e)}. ƒåast√© u singul√°rn√≠ch matic (nap≈ô. perfektn√≠ korelace).")
    except Exception as e:
         logger.error(f"Neoƒçek√°van√° chyba p≈ôi fitov√°n√≠ modelu Factor Analysis: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"Neoƒçek√°van√° chyba p≈ôi fitov√°n√≠ modelu Factor Analysis: {e}")


    # --- Extract Results ---
    try:
        loadings = fa.loadings_ # NumPy array (n_variables, n_factors)
        variance_info = fa.get_factor_variance() # Tuple: (SSL, Percent Var, Cumulative Var)
        communalities_array = fa.get_communalities() # NumPy array

    except Exception as e:
        logger.error(f"Chyba p≈ôi extrakci v√Ωsledk≈Ø z FA modelu: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Chyba p≈ôi z√≠sk√°v√°n√≠ v√Ωsledk≈Ø z analyz√°toru faktor≈Ø.")

    # --- Format Results ---
    factor_names = [f"Factor{i+1}" for i in range(n_factors_extracted)] # Changed name to match frontend

    # Factor Loadings
    loadings_dict: Dict[str, Dict[str, Optional[float]]] = {}
    variable_names = data_to_analyze.columns.tolist() # Use columns from the analyzed data
    for i, var_name in enumerate(variable_names):
        loadings_dict[var_name] = {}
        for j, factor_name in enumerate(factor_names):
             # Ensure loadings array dimensions match expected factors/variables
             if i < loadings.shape[0] and j < loadings.shape[1]:
                 loadings_dict[var_name][factor_name] = safe_float(loadings[i, j])
             else:
                 loadings_dict[var_name][factor_name] = None # Or handle as error
                 logger.warning(f"Nesoulad dimenz√≠ v loadings pro {var_name}, {factor_name}")


    # Factor Variance
    variance_list: List[FactorVarianceItem] = []
    total_variance_explained = None
    if variance_info and len(variance_info) == 3 and len(variance_info[0]) == n_factors_extracted:
        ssl_values, var_pct_values, cum_var_pct_values = variance_info
        for j, factor_name in enumerate(factor_names):
            ssl = safe_float(ssl_values[j])
            var_pct = safe_float(var_pct_values[j] * 100)
            cum_var_pct = safe_float(cum_var_pct_values[j] * 100)

            variance_list.append(FactorVarianceItem(
                factor=factor_name,
                ssl=ssl if ssl is not None else 0.0,
                variance_pct=var_pct if var_pct is not None else 0.0,
                cumulative_variance_pct=cum_var_pct if cum_var_pct is not None else 0.0,
            ))
        if variance_list:
             total_variance_explained = variance_list[-1].cumulative_variance_pct # Get from last item
    else:
         logger.warning("Nepoda≈ôilo se z√≠skat validn√≠ informace o varianci faktor≈Ø.")


    # Communalities
    communalities_dict: Dict[str, Optional[float]] = {}
    if communalities_array is not None and len(communalities_array) == len(variable_names):
        for i, var_name in enumerate(variable_names):
            communalities_dict[var_name] = safe_float(communalities_array[i])
    else:
         logger.warning("Nepoda≈ôilo se z√≠skat validn√≠ informace o komunalit√°ch nebo nesed√≠ d√©lka.")
         for var_name in variable_names:
             communalities_dict[var_name] = None # Fill with None if extraction failed


    # --- Construct Final Result Object ---
    result = FactorAnalysisResult(
        columns_used=variable_names, # Use the actual columns used after cleaning/preprocessing
        n_factors_requested=request.n_factors,
        n_factors_extracted=n_factors_extracted,
        eigenvalue_criterion_used=eigenvalue_criterion_used,
        eigenvalues=eigenvalues_list if eigenvalue_criterion_used else None,
        # Use 'Bez rotace' if rotation was 'none'/'None', else use the requested rotation name
        rotation_used="Bez rotace" if rotation_param is None else request.rotation,
        standardized=request.standardize,
        dropped_rows=dropped_rows,
        data_adequacy=adequacy_results,
        factor_loadings=loadings_dict,
        factor_variance=variance_list,
        total_variance_explained_pct=safe_float(total_variance_explained), # Ensure it's safe float
        communalities=communalities_dict,
    )

    logger.info("Faktorov√° anal√Ωza √∫spƒõ≈°nƒõ dokonƒçena.")
    return result

@app.get("/api/get_stored_data")
async def get_stored_data():
    if "latest" not in stored_datasets:
        return {"headers": [], "data": []}
    return stored_datasets["latest"]


@app.post("/api/fill_missing")
async def fill_missing(request: Request):
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=400, detail="Data nejsou nahran√°.")

    body = await request.json()
    strategies: dict = body.get("strategies", {})

    headers = stored_datasets["latest"]["headers"]
    raw_data = stored_datasets["latest"]["data"]
    prev_mask = stored_datasets["latest"].get("filled_mask")

    df = pd.DataFrame(raw_data, columns=headers)

    # üßº Zru≈°en√≠ p≈ôedchoz√≠ch doplnƒõn√Ωch hodnot
    if prev_mask:
        prev_mask_np = np.array(prev_mask)
        for row_idx in range(len(df)):
            for col_idx in range(len(headers)):
                if prev_mask_np[row_idx][col_idx]:
                    df.iat[row_idx, col_idx] = None

    filled_mask = pd.DataFrame(False, index=df.index, columns=df.columns)

    for col, method in strategies.items():
        if col not in df.columns:
            continue

        # üìå V≈ædy resetuj chybƒõj√≠c√≠ hodnoty na None p≈ôed v√Ωpoƒçtem
        mask = df[col].isna() | (df[col] == "") | (df[col].astype(str).str.lower() == "nan")
        df.loc[mask, col] = None
        original = df[col].copy()

        if method == "mean":
            value = pd.to_numeric(df[col], errors='coerce').mean()
            df.loc[mask, col] = value

        elif method == "median":
            value = pd.to_numeric(df[col], errors='coerce').median()
            df.loc[mask, col] = value

        elif method == "mode":
            mode_series = df[col].mode()
            if not mode_series.empty:
                df.loc[mask, col] = mode_series[0]

        elif method == "zero":
            df.loc[mask, col] = 0

        elif method == "drop":
            df = df[~mask]
            filled_mask = filled_mask.loc[df.index]
            continue

        elif method == "ffill":
            df[col] = df[col].replace("", pd.NA).fillna(method='ffill').fillna(method='bfill')

        elif method == "bfill":
            df[col] = df[col].replace("", pd.NA).fillna(method='bfill').fillna(method='ffill')

        elif method == "interpolate":
            df[col] = pd.to_numeric(df[col], errors='coerce').interpolate(method='linear', limit_direction='both')

        # Oznaƒç zmƒõny v masce
        filled_mask[col] = mask & (df[col] != original)

    # üì¶ Ulo≈æ v√Ωsledek
    stored_datasets["latest"] = {
        "headers": df.columns.tolist(),
        "data": df.astype(str).where(pd.notna(df), "").values.tolist(),
        "filled_mask": filled_mask.values.tolist()
    }

    return {
        "status": "filled",
        "filled_cells": int(filled_mask.values.sum()),
        "headers": df.columns.tolist(),
        "data": df.astype(str).where(pd.notna(df), "").values.tolist(),
        "filled_mask": filled_mask.values.tolist()
    }


@app.post("/api/analyze")
async def analyze_data(request: DataRequest):
    logger.info("Received request for /api/analyze")
    column_types_result = {}

    try:
        # --- MINIMALISTICK√Å ZMƒöNA: Zajistit existenci 'latest' ---
        if "latest" not in stored_datasets:
            logger.info("Creating 'latest' key in stored_datasets.")
            stored_datasets["latest"] = {}
        # --- KONEC MINIMALISTICK√â ZMƒöNY ---

        # Validace vstupu
        if not request.headers:
             logger.warning("/api/analyze: Received empty headers.")
             stored_datasets["latest"]["column_types"] = {} # Ulo≈æit pr√°zdn√Ω, aby kl√≠ƒç existoval
             return {"column_types": {}}

        df = pd.DataFrame(request.data or [], columns=request.headers)
        logger.info(f"/api/analyze: DataFrame shape {df.shape}")

        if df.empty and request.headers:
             logger.warning("/api/analyze: DataFrame is empty but headers exist.")
             # Vytvo≈ô√≠me z√°znamy s nezn√°m√Ωm typem pro hlaviƒçky
             for col in request.headers:
                 column_types_result[col] = {"type": "Nezn√°m√Ω", "missing": "100.00%"}
             stored_datasets["latest"]["column_types"] = column_types_result
             return {"column_types": column_types_result}
        elif df.empty:
             logger.warning("/api/analyze: DataFrame is empty and no headers.")
             stored_datasets["latest"]["column_types"] = {}
             return {"column_types": {}}


        df = df.replace(['', None], np.nan)

        missing_data = df.isna().mean() * 100
        total_rows = len(df)

        for column in request.headers:
            if column not in df.columns:
                 column_types_result[column] = {"type": "Nezn√°m√Ω", "missing": "100.00%"}
                 continue

            series = df[column].dropna()
            unique_values = series.nunique()

            # Va≈°e logika detekce prahu
            if total_rows < 200: category_threshold = 8
            elif total_rows < 1000: category_threshold = 10
            else: category_threshold = 15

            # Va≈°e logika detekce typu
            is_numeric = pd.to_numeric(series, errors='coerce').notna().all() if not series.empty else False # P≈ôid√°na kontrola na pr√°zdnou s√©rii
            detected_type = "Kategorie"
            if is_numeric:
                if unique_values > category_threshold:
                    detected_type = "ƒå√≠seln√Ω"

            column_types_result[column] = {
                "type": detected_type,
                "missing": f"{missing_data.get(column, 0):.2f}%"
            }

    except Exception as e:
        logger.error(f"Error during /api/analyze: {e}", exc_info=True)
        # I p≈ôi chybƒõ zajist√≠me existenci kl√≠ƒçe
        if "latest" not in stored_datasets:
            stored_datasets["latest"] = {}
        stored_datasets["latest"]["column_types"] = {} # Ulo≈æit pr√°zdn√Ω
        raise HTTPException(status_code=500, detail=f"Chyba p≈ôi anal√Ωze dat: {str(e)}")

    # --- Ulo≈æen√≠ v√Ωsledku ---
    # Kl√≠ƒç 'latest' by u≈æ mƒõl existovat z kontroly na zaƒç√°tku
    stored_datasets["latest"]["column_types"] = column_types_result
    logger.info(f"/api/analyze completed. Stored/Updated 'column_types' for keys: {list(column_types_result.keys())}")

    return {"column_types": column_types_result}

class RecalculateSingleNormalityRequest(BaseModel):
    column: str      # N√°zev sloupce k p≈ôepoƒçtu
    test_method: str # Po≈æadovan√° metoda ('shapiro' nebo 'ks')

class NormalityResultModel(BaseModel):
    column: str
    test: str
    pValue: float
    isNormal: bool
    warning: str
    hasMissing: bool

@app.post("/api/recalculate_single_normality", response_model=NormalityResultModel)
async def recalculate_single_column_normality(request: RecalculateSingleNormalityRequest):
    """
    Zcela samostatnƒõ p≈ôepoƒç√≠t√° normalitu pro JEDEN specifick√Ω sloupec
    s pou≈æit√≠m explicitnƒõ zadan√© testovac√≠ metody.
    Naƒç√≠t√° si data znovu a neovliv≈àuje ostatn√≠ endpointy.
    """
    column_name = request.column
    test_method = request.test_method

    # 1. Z√≠sk√°n√≠ dat (stejnƒõ jako by to dƒõlal jin√Ω endpoint)
    if "latest" not in stored_datasets:
        raise HTTPException(status_code=404, detail="Data nejsou nahran√° v pamƒõti serveru.")

    dataset_info = stored_datasets["latest"]
    headers = dataset_info.get("headers")
    data = dataset_info.get("data")

    if not headers or not data:
         raise HTTPException(status_code=400, detail="V ulo≈æen√Ωch datech chyb√≠ hlaviƒçky nebo data.")

    # 2. Validace vstup≈Ø specifick√Ωch pro tento po≈æadavek
    if column_name not in headers:
         raise HTTPException(status_code=404, detail=f"Po≈æadovan√Ω sloupec '{column_name}' nebyl v datech nalezen.")
    if test_method not in ["shapiro", "ks"]:
        raise HTTPException(status_code=400, detail="Nezn√°m√° testovac√≠ metoda. Povolen√© hodnoty jsou 'shapiro' nebo 'ks'.")

    # 3. Zpracov√°n√≠ dat pro dan√Ω sloupec (izolovanƒõ)
    try:
        df = pd.DataFrame(data, columns=headers)
        if column_name not in df.columns:
             # Dvojit√° kontrola pro jistotu, i kdy≈æ u≈æ m√°me kontrolu v headers
             raise HTTPException(status_code=404, detail=f"Sloupec '{column_name}' se nepoda≈ôilo naj√≠t v DataFrame.")

        original_series = df[column_name] # Z√≠sk√°me p≈Øvodn√≠ s√©rii
        # Pokus o konverzi na numerick√Ω typ, nevalidn√≠ hodnoty budou NaN
        numeric_series_coerced = pd.to_numeric(original_series, errors='coerce')
        # Odstran√≠me NaN hodnoty pro samotn√Ω test
        series_cleaned = numeric_series_coerced.dropna()

        # 4. Z√°kladn√≠ kontroly na vyƒçi≈°tƒõn√Ωch datech
        if not pd.api.types.is_numeric_dtype(series_cleaned):
             # Pokud ani po dropna nen√≠ numerick√Ω (nap≈ô. p≈Øvodnƒõ jen text), nem≈Ø≈æeme testovat
             raise HTTPException(status_code=400, detail=f"Sloupec '{column_name}' neobsahuje ƒç√≠seln√° data nebo jen chybƒõj√≠c√≠ hodnoty.")
        if series_cleaned.empty:
             raise HTTPException(status_code=400, detail=f"Sloupec '{column_name}' neobsahuje ≈æ√°dn√© platn√© ƒç√≠seln√© hodnoty po odstranƒõn√≠ chybƒõj√≠c√≠ch.")
        if len(series_cleaned) < 3:
            # Oba testy vy≈æaduj√≠ alespo≈à 3 body
            raise HTTPException(status_code=400, detail=f"Nedostatek platn√Ωch hodnot (< 3) ve sloupci '{column_name}' pro proveden√≠ testu '{test_method}'. Nalezeno: {len(series_cleaned)}.")

        # 5. V√Ωpoƒçet pomocn√Ωch informac√≠ (chybƒõj√≠c√≠ hodnoty, outliery)
        total_count = len(original_series)
        missing_count = numeric_series_coerced.isna().sum() # Poƒçet NaN po pokusu o konverzi
        has_missing = missing_count > 0

        outlier_note = ""
        if len(series_cleaned) > 1: # Pot≈ôebujeme alespo≈à 2 body pro std dev
            std_dev = series_cleaned.std(ddof=1)
            if std_dev is not None and not np.isnan(std_dev) and std_dev > 0:
                z_scores = zscore(series_cleaned, ddof=1)
                outlier_ratio = np.mean(np.abs(z_scores) > 3)
                if outlier_ratio > 0.05:
                    outlier_note = "Pozor: detekov√°no >5% potenci√°ln√≠ch outlier≈Ø (Z-sk√≥re > 3), m≈Ø≈æe ovlivnit v√Ωsledek."
            elif std_dev == 0:
                 outlier_note = "Pozn: V≈°echny platn√© hodnoty jsou stejn√©."
            # else: std_dev je None nebo NaN - nem≈Ø≈æeme poƒç√≠tat z-scores

        # 6. Proveden√≠ vy≈æ√°dan√©ho testu normality
        test_used = ""
        p_value_float = float('nan') # Defaultn√≠ hodnota pro p≈ô√≠pad chyby
        reason = ""
        stat = None

        try:
            if test_method == "shapiro":
                if len(series_cleaned) > 5000:
                    # Fallback pro Shapiro-Wilk nad 5000 hodnot
                    standardized = (series_cleaned - series_cleaned.mean()) / series_cleaned.std(ddof=1)
                    stat, p = kstest(standardized, "norm")
                    p_value_float = float(f"{p:.6g}")
                    test_used = "Kolmogorov‚ÄìSmirnov (fallback z Shapiro >5000)"
                    reason = f"Explicitnƒõ vy≈æ√°d√°n Shapiro-Wilk, ale poƒçet hodnot (N={len(series_cleaned)}) > 5000. Pou≈æit K-S."
                else:
                    # Standardn√≠ Shapiro-Wilk
                    stat, p = shapiro(series_cleaned)
                    p_value_float = float(f"{p:.6g}")
                    test_used = "Shapiro-Wilk"
                    reason = f"Explicitnƒõ vy≈æ√°d√°n test {test_used} (N={len(series_cleaned)})."

            elif test_method == "ks":
                # Kolmogorov-Smirnov test
                standardized = (series_cleaned - series_cleaned.mean()) / series_cleaned.std(ddof=1)
                stat, p = kstest(standardized, "norm")
                p_value_float = float(f"{p:.6g}")
                test_used = "Kolmogorov‚ÄìSmirnov"
                reason = f"Explicitnƒõ vy≈æ√°d√°n test {test_used} (N={len(series_cleaned)})."

        except ValueError as ve:
            # Chyba p≈ô√≠mo v testovac√≠ funkci (nap≈ô. teoreticky N<3 i kdy≈æ jsme kontrolovali)
             raise HTTPException(status_code=400, detail=f"Chyba p≈ôi prov√°dƒõn√≠ testu '{test_method}' pro sloupec '{column_name}': {ve}")

        # 7. Sestaven√≠ fin√°ln√≠ pozn√°mky (warning)
        note_parts = [reason] # Zaƒçneme d≈Øvodem v√Ωbƒõru testu
        if outlier_note:
             note_parts.append(outlier_note)
        if has_missing:
            note_parts.append(f"P≈Øvodn√≠ sloupec obsahuje {missing_count} chybƒõj√≠c√≠ch/neƒç√≠seln√Ωch hodnot z {total_count}.")
        final_warning = " ".join(note_parts) if note_parts else "-"

        # 8. Sestaven√≠ a vr√°cen√≠ v√Ωsledku
        is_normal = bool(not np.isnan(p_value_float) and p_value_float > 0.05)

        return NormalityResultModel(
            column=column_name,
            test=test_used,
            pValue=p_value_float,
            isNormal=is_normal,
            warning=final_warning,
            hasMissing=has_missing
        )

    except HTTPException:
        # Pokud byla vyhozena HTTPException u≈æ d≈ô√≠ve (validace vstup≈Ø, dat),
        # nech√°me ji proj√≠t d√°l.
        raise
    except Exception as e:
        # Z√°chyt jak√©koli jin√© neoƒçek√°van√© chyby bƒõhem zpracov√°n√≠
        print(f"Neoƒçek√°van√° chyba p≈ôi p≈ôepoƒçtu normality pro sloupec {column_name} metodou {test_method}: {e}")
        # M≈Ø≈æeme zde logovat cel√© traceback: import traceback; traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Intern√≠ serverov√° chyba p≈ôi p≈ôepoƒçtu normality pro sloupec '{column_name}'. Detail: {e}")